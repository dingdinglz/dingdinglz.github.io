---
title: dinglzå¸¦ä½ è¯»deepcode
description: ä»Šå¤©è¦è¯»çš„é¡¹ç›®æ˜¯ï¼šhttps://github.com/HKUDS/DeepCode
date: 2026-01-18 02:00:00+0000
categories:
    - coderead
tags:
    - Python
    - Agent
---

ä»Šå¤©æˆ‘ä»¬è¦çœ‹çš„é¡¹ç›®æ˜¯ï¼šhttps://github.com/HKUDS/DeepCode ï¼Œæˆ‘è¯»çš„commitæ˜¯mainåˆ†æ”¯çš„ab730642731d9c8a400cad78e76962253ac875f2

è·Ÿç€æœ¬ç¯‡blogè¯»repoå»ºè®®æŠŠä»£ç æ‹‰ä¸‹æ¥ï¼Œåˆ‡åˆ°ç›¸åŒçš„commitï¼Œè·Ÿç€æµç¨‹ä¸€èµ·æ¥è¯»

## æ¦‚è¿°

DeepCode æ˜¯ä¸€ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å¼€æºæ¡†æ¶ï¼Œæ—¨åœ¨å°†ç ”ç©¶è®ºæ–‡ã€è‡ªç„¶è¯­è¨€æè¿°ç­‰è¾“å…¥è‡ªåŠ¨è½¬æ¢ä¸ºç”Ÿäº§çº§ä»£ç ã€‚è¯¥é¡¹ç›®é€šè¿‡è‡ªåŠ¨åŒ–ç®—æ³•å®ç°ã€å‰ç«¯å¼€å‘å’Œåç«¯æ„å»ºï¼Œæ˜¾è‘—æå‡äº†ç ”å‘æ•ˆç‡ã€‚å…¶æ ¸å¿ƒç›®æ ‡æ˜¯è§£å†³ç ”ç©¶äººå‘˜åœ¨å®ç°å¤æ‚ç®—æ³•æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå‡å°‘å¼€å‘å»¶è¿Ÿï¼Œå¹¶é¿å…é‡å¤æ€§ç¼–ç å·¥ä½œã€‚

DeepCode æ”¯æŒå¤šç§è¾“å…¥å½¢å¼ï¼ŒåŒ…æ‹¬å­¦æœ¯è®ºæ–‡ã€æ–‡æœ¬æç¤ºã€URL å’Œæ–‡æ¡£æ–‡ä»¶ï¼ˆå¦‚ PDFã€DOCã€PPTXã€TXTã€HTMLï¼‰ï¼Œå¹¶èƒ½ç”Ÿæˆé«˜è´¨é‡ã€å¯æ‰©å±•ä¸”åŠŸèƒ½ä¸°å¯Œçš„ä»£ç ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨å¤šæ™ºèƒ½ä½“åä½œæ¶æ„ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„å¼€å‘ä»»åŠ¡ï¼Œä»æ¦‚å¿µåˆ°å¯éƒ¨ç½²çš„åº”ç”¨ç¨‹åºã€‚

```mermaid
flowchart LR
A["ğŸ“„ ç ”ç©¶è®ºæ–‡<br/>ğŸ’¬ æ–‡æœ¬æç¤º<br/>ğŸŒ URLs & æ–‡æ¡£<br/>ğŸ“ æ–‡ä»¶: PDF, DOC, PPTX, TXT, HTML"] --> B["ğŸ§  DeepCode<br/>å¤šæ™ºèƒ½ä½“å¼•æ“"]
B --> C["ğŸš€ ç®—æ³•å®ç° <br/>ğŸ¨ å‰ç«¯å¼€å‘ <br/>âš™ï¸ åç«¯å¼€å‘"]
style A fill:#ff6b6b,stroke:#c0392b,stroke-width:2px,color:#000
style B fill:#00d4ff,stroke:#0984e3,stroke-width:3px,color:#000
style C fill:#00b894,stroke:#00a085,stroke-width:2px,color:#000
```

## å…¥å£

è¦è¯»æ‡‚å…¨æµç¨‹ï¼Œå…ˆæ‰¾å…¥å£ï¼Œå¼€å§‹äºï¼š

``` bash
python cli/main_cli.py --file paper.pdf
```

æ‰€ä»¥æˆ‘ä»¬å…ˆæ¥çœ‹`cli/main_cli.py`ã€‚

æ‰¾åˆ°mainå‡½æ•°ï¼Œè·³è¿‡ç¯å¢ƒæ£€æŸ¥ç­‰ç­‰ï¼Œä½ ä¼šå‘ç°æœ€æ ¸å¿ƒçš„åœ°æ–¹åœ¨ï¼š

``` python
if args.file or args.url or args.chat:
    if args.file:
        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(args.file):
            print(f"{Colors.FAIL}âŒ File not found: {args.file}{Colors.ENDC}")
            sys.exit(1)
        success = await run_direct_processing(app, args.file, "file")
    elif args.url:
        success = await run_direct_processing(app, args.url, "url")
    elif args.chat:
        # éªŒè¯chatè¾“å…¥é•¿åº¦
        if len(args.chat.strip()) < 20:
            print(
                f"{Colors.FAIL}âŒ Chat input too short. Please provide more detailed requirements (at least 20 characters){Colors.ENDC}"
            )
            sys.exit(1)
        success = await run_direct_processing(app, args.chat, "chat")

    sys.exit(0 if success else 1)
else:
    # äº¤äº’å¼æ¨¡å¼
    print(f"\n{Colors.CYAN}ğŸ® Starting interactive mode...{Colors.ENDC}")
    await app.run_interactive_session()
```

ä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬åªéœ€è¦çœ‹`run_direct_processing`å‡½æ•°å°±okäº†

æ¥çœ‹ï¼šè¿™ä¸ªå‡½æ•°é‡Œçš„æ ¸å¿ƒä»£ç 

``` python
# åˆå§‹åŒ–åº”ç”¨
init_result = await app.initialize_mcp_app()
if init_result["status"] != "success":
    print(
        f"{Colors.FAIL}âŒ Initialization failed: {init_result['message']}{Colors.ENDC}"
    )
    return False

# å¤„ç†è¾“å…¥
result = await app.process_input(input_source, input_type)
```

è¦å…³æ³¨çš„å…¶å®ä¹Ÿå°±æ˜¯ï¼š`app.initialize_mcp_app()`å’Œ`app.process_input(input_source, input_type)`ï¼Œè¿˜è®°å¾—input_sourceå’Œinput_typeå—ï¼Œinput_typeæœ‰chatã€urlã€fileï¼Œsourceå°±æ˜¯å¯¹åº”çš„å†…å®¹

é‚£æˆ‘ä»¬ä¸‹é¢åˆ†åˆ«æ¥çœ‹`app.initialize_mcp_app()`å’Œ`app.process_input(input_source, input_type)`ï¼Œè®°å½•ä¸€ä¸‹ç°åœ¨çš„taskList

---

- [ ] è¯»`app.initialize_mcp_app()`
- [ ] è¯»`app.process_input(input_source, input_type)`

---

## mcpéƒ¨åˆ†

æ¥çœ‹`app.initialize_mcp_app()`ï¼Œæœ€ç»ˆå®šä½åˆ°

``` python
async def initialize_mcp_app(self):
    """åˆå§‹åŒ–MCPåº”ç”¨ - ä½¿ç”¨å·¥ä½œæµé€‚é…å™¨"""
    # Workflow adapter will handle MCP initialization
    return await self.workflow_adapter.initialize_mcp_app()
```

æ¥çœ‹`self.workflow_adapter.initialize_mcp_app()`ï¼Œæ ¸å¿ƒä»£ç ï¼š

``` python
# Initialize MCP application
self.app = MCPApp(name="cli_agent_orchestration")
self.app_context = self.app.run()
agent_app = await self.app_context.__aenter__()
```

è¯¶ï¼ŸåŸæ¥ä¸æ˜¯åˆå§‹åŒ–mcpçš„é…ç½®ï¼Œè€Œæ˜¯ç›´æ¥åˆå§‹åŒ–äº†self.appä¹Ÿå°±æ˜¯æ ¸å¿ƒçš„agentï¼Œä»–è¿™é‡Œagentå°±å«åšmcp applicationï¼Œæ€»ä¹‹è¿™é‡Œå…ˆæŠŠcliçš„appåˆå§‹åŒ–æˆäº†cli_agent_orchestrationï¼Œå¯ä»¥ç»™å¤§å®¶çœ‹ä¸€ä¸‹è¿™è¾¹agentçš„æ¶æ„å›¾ï¼š

```mermaid
graph TB
subgraph "æ™ºèƒ½ä½“å±‚"
CO[ä¸­å¤®åè°ƒæ™ºèƒ½ä½“]
IU[æ„å›¾ç†è§£æ™ºèƒ½ä½“]
DP[æ–‡æ¡£è§£ææ™ºèƒ½ä½“]
CP[ä»£ç è§„åˆ’æ™ºèƒ½ä½“]
CR[ä»£ç å‚è€ƒæŒ–æ˜æ™ºèƒ½ä½“]
CI[ä»£ç ç”Ÿæˆæ™ºèƒ½ä½“]
end
subgraph "MCPæœåŠ¡å™¨å±‚"
DS[document-segmentation]
FI[filesystem]
BR[brave]
GH[github-downloader]
CI_S[command-executor]
CI_I[Code Implementation Server]
end
CO --> |è°ƒåº¦| IU
CO --> |è°ƒåº¦| DP
CO --> |è°ƒåº¦| CP
CO --> |è°ƒåº¦| CR
CO --> |è°ƒåº¦| CI
DP --> |è°ƒç”¨| DS
CR --> |è°ƒç”¨| BR
CR --> |è°ƒç”¨| GH
CI --> |è°ƒç”¨| FI
CI --> |è°ƒç”¨| CI_S
CI --> |è°ƒç”¨| CI_I
```

åˆšåˆšåˆå§‹åŒ–çš„å°±æ˜¯ä¸­å¤®åè°ƒæ™ºèƒ½ä½“ï¼Œè®©æˆ‘ä»¬æ›´æ–°ä¸€ä¸‹tasklist

---

- [x] è¯»`app.initialize_mcp_app()`
- [ ] è¯»`app.process_input(input_source, input_type)`

---

é‚£æˆ‘ä»¬æ¥ç€æ¥çœ‹ï¼š`app.process_input(input_source, input_type)`

## app.process_input(input_source, input_type)

è¿™é‡Œæœ‰æ®µæŒºå¥‡æ€ªçš„ä»£ç ï¼š

``` python
# Update segmentation configuration before processing
# åœ¨å¤„ç†ä¹‹å‰æ›´æ–°segmentation configuration
self.update_segmentation_config()
```

çœ‹ä¸‹è¿™ä¸ªå‡½æ•°ï¼š

``` python
def update_segmentation_config(self):
    """Update document segmentation configuration in mcp_agent.config.yaml"""
    import yaml
    import os

    config_path = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        "mcp_agent.config.yaml",
    )

    try:
        # Read current config
        with open(config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)

        # Update document segmentation settings
        if "document_segmentation" not in config:
            config["document_segmentation"] = {}

        config["document_segmentation"]["enabled"] = self.segmentation_config[
            "enabled"
        ]
        config["document_segmentation"]["size_threshold_chars"] = (
            self.segmentation_config["size_threshold_chars"]
        )

        # Write updated config
        with open(config_path, "w", encoding="utf-8") as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)

        self.cli.print_status(
            "ğŸ“„ Document segmentation configuration updated", "success"
        )

    except Exception as e:
        self.cli.print_status(
            f"âš ï¸ Failed to update segmentation config: {str(e)}", "warning"
        )
```

å‘ç°å…¶å®å°±æ˜¯å…ˆåŠ è½½é…ç½®ï¼Œç„¶ååŒæ­¥cliçš„é…ç½®

å†å›æ¥çœ‹ï¼Œç„¶åè¿™ä¸ªå‡½æ•°çš„æ ¸å¿ƒå°±åœ¨äºï¼š

``` python
# ä½¿ç”¨å·¥ä½œæµé€‚é…å™¨è¿›è¡Œå¤„ç†
result = await self.workflow_adapter.process_input_with_orchestration(
    input_source=input_source,
    input_type=input_type,
    enable_indexing=self.cli.enable_indexing,
)
```

è¿›åˆ°è¿™ä¸ªå‡½æ•°é‡Œçœ‹çœ‹ï¼Œå‰é¢éƒ½æ˜¯é¢„å¤„ç†ï¼Œæœ€æ ¸å¿ƒçš„ä»£ç åœ¨

``` python
# Execute appropriate pipeline based on input type
if input_type == "chat":
    # Use chat-based planning pipeline for user requirements
    pipeline_result = await self.execute_chat_pipeline(input_source)
else:
    # Use traditional multi-agent research pipeline for files/URLs
    pipeline_result = await self.execute_full_pipeline(
        input_source, enable_indexing=enable_indexing
    )
```

æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹fileå’Œurlä¸‹çš„å…¨æµç¨‹ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦çœ‹ä¸‹`self.execute_full_pipeline`å°±è¡Œäº†ï¼Œæ›´æ–°ä¸€ä¸‹tasklist

---

- [x] è¯»`app.initialize_mcp_app()`
- [x] è¯»`app.process_input(input_source, input_type)`

---

å¹¶è¡Œä»»åŠ¡ç»“æŸäº†ï¼Œæˆ‘ä»¬ä¸‹é¢æ¥è¯»`self.execute_full_pipeline`ã€‚

## self.execute_full_pipeline

è¯»ä¸€ä¸‹å‘ç°è¿™ä¸ªå‡½æ•°æœ€æ ¸å¿ƒçš„åœ°æ–¹å°±åœ¨ï¼š

``` python
result = await execute_multi_agent_research_pipeline(
    input_source=input_source,
    logger=self.logger,
    progress_callback=progress_callback,
    enable_indexing=enable_indexing,
)
```

é‚£æˆ‘ä»¬è¿›åˆ°`execute_multi_agent_research_pipeline`é‡Œï¼Œå¯ä»¥çœ‹å‡ºæ¥æˆ‘ä»¬ç¦»æ ¸å¿ƒä»£ç è¶Šæ¥è¶Šè¿‘äº†

## execute_multi_agent_research_pipeline

å…ˆçœ‹ä¸€ä¸‹è¿™ä¸ªå‡½æ•°çš„æ³¨é‡Šå§ï¼š

```
Execute the complete intelligent multi-agent research orchestration pipeline.

This is the main AI orchestration engine that coordinates autonomous research workflow agents:
- Local workspace automation for seamless environment management
- Intelligent research analysis with automated content processing
- AI-driven code architecture synthesis and design automation
- Reference intelligence discovery with automated knowledge extraction (optional)
- Codebase intelligence orchestration with automated relationship analysis (optional)
- Intelligent code implementation synthesis with AI-powered development
```

ç¿»è¯‘æ¥å–½ï¼š

```
æ‰§è¡Œå®Œæ•´çš„æ™ºèƒ½å¤šæ™ºèƒ½ä½“ç ”ç©¶ç¼–æ’æµç¨‹ã€‚ 

è¿™æ˜¯åè°ƒè‡ªä¸»ç ”ç©¶å·¥ä½œæµæ™ºèƒ½ä½“çš„ä¸»è¦AIç¼–æ’å¼•æ“ï¼ŒåŒ…æ‹¬ï¼š 

- æœ¬åœ°å·¥ä½œåŒºè‡ªåŠ¨åŒ–ï¼Œå®ç°æ— ç¼ç¯å¢ƒç®¡ç†
- æ™ºèƒ½ç ”ç©¶åˆ†æï¼Œç»“åˆè‡ªåŠ¨åŒ–å†…å®¹å¤„ç†
- AIé©±åŠ¨çš„ä»£ç æ¶æ„ç»¼åˆä¸è®¾è®¡è‡ªåŠ¨åŒ–
- å‚è€ƒæ™ºèƒ½å‘ç°ï¼Œç»“åˆè‡ªåŠ¨åŒ–çŸ¥è¯†æå–ï¼ˆå¯é€‰ï¼‰
- ä»£ç åº“æ™ºèƒ½ç¼–æ’ï¼Œç»“åˆè‡ªåŠ¨åŒ–å…³ç³»åˆ†æï¼ˆå¯é€‰ï¼‰
- æ™ºèƒ½ä»£ç å®ç°ç»¼åˆï¼Œç»“åˆAIé©±åŠ¨çš„å¼€å‘
```

é‚£æˆ‘ä»¬ç»§ç»­å¾€ä¸‹è¯»ä»£ç ï¼š

æºä»£ç ä¸‹é¢çš„éƒ¨åˆ†åˆ—çš„ä¹Ÿæ¯”è¾ƒè¯¦ç»†ï¼Œåˆ†äº†phase 0ã€1ã€2ã€3ï¼Œæˆ‘ä»¬ä¹Ÿè·Ÿç€å®ƒçš„æ€è·¯æ¥çœ‹

### phase0

phase0æ˜¯åˆå§‹åŒ–å·¥ä½œåŒºï¼Œæ ¸å¿ƒä»£ç æ˜¯ï¼š

``` python
# Setup local workspace directory
workspace_dir = os.path.join(os.getcwd(), "deepcode_lab")
os.makedirs(workspace_dir, exist_ok=True)
```

å…¶å®å°±æ˜¯æ–°å»ºäº†æ–‡ä»¶å¤¹`deepcode_lab`

### phase1

å¤„ç†å’ŒéªŒè¯è¾“å…¥ï¼Œè®©æˆ‘ä»¬è¿›æ¥çœ‹ï¼š`_process_input_source`

``` python
if input_source.startswith("file://"):
    file_path = input_source[7:]
    if os.name == "nt" and file_path.startswith("/"):
        file_path = file_path.lstrip("/")
    return file_path
return input_source
```

å‘ç°åªæ˜¯æŠŠ`file://`å¼€å¤´çš„æ–‡ä»¶ç›®å½•æ”¹æˆäº†æ­£å¸¸çš„æ–‡ä»¶ç›®å½•ï¼Œç»§ç»­çœ‹phase2

### phase2

``` python
# Phase 2: Research Analysis and Resource Processing (if needed)
if isinstance(input_source, str) and (
        input_source.endswith((".pdf", ".docx", ".txt", ".html", ".md"))
        or input_source.startswith(("http", "file://"))
):
    (
        analysis_result,
        download_result,
    ) = await orchestrate_research_analysis_agent(
        input_source, logger, progress_callback
    )
else:
    download_result = input_source  # Use input directly if already processed
```

è¿™ä¸€æ®µä¸»è¦æ˜¯è§£æ`input_source`ï¼Œå¦‚æœä»–æ˜¯æ–‡ä»¶æˆ–è€…urlå°±è¿è¡Œ`orchestrate_research_analysis_agent`è·å–`analysis_result`å’Œ`download_result`ï¼Œå¦‚æœéƒ½ä¸æ˜¯å°±ç›´æ¥ä½œä¸ºè¾“å…¥å†…å®¹ï¼Œä½œä¸ºè¾“å…¥å†…å®¹å¯èƒ½æ˜¯ä¸ºäº†chatè®¾è®¡çš„ï¼Œæˆ‘ä»¬è¿›æ¥çœ‹`orchestrate_research_analysis_agent`ã€‚

#### orchestrate_research_analysis_agent

è€æ ·å­ï¼Œå…ˆçœ‹æ³¨é‡Šï¼š

```
Orchestrate intelligent research analysis and resource processing automation.

This agent coordinates multiple AI components to analyze research content
and process associated resources with automated workflow management.
```

ä¸­æ–‡ï¼š

```
ç¼–æ’æ™ºèƒ½ç ”ç©¶åˆ†æä¸èµ„æºå¤„ç†è‡ªåŠ¨åŒ–ã€‚ 

æ­¤æ™ºèƒ½ä½“åè°ƒå¤šä¸ªAIç»„ä»¶ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–å·¥ä½œæµç®¡ç†ï¼Œä»¥åˆ†æç ”ç©¶å†…å®¹å¹¶å¤„ç†ç›¸å…³èµ„æºã€‚ 
```

è¿™ä¸€æ®µç®—æ˜¯è¯¥é¡¹ç›®ä¸­ç›¸å½“é‡è¦çš„ä¸€éƒ¨åˆ†äº†ï¼Œè§£æpdfèµ„æºä¹Ÿæ˜¯æˆ‘éå¸¸å¥½å¥‡å®ƒå¦‚ä½•å¤„ç†çš„åœ°æ–¹ï¼Œåé¢å¦‚æœæœ‰å¤„ç†èµ„æºçš„éƒ¨åˆ†æˆ‘ä»¬ä¹Ÿåªæ¥çœ‹pdfèµ„æºçš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä¸å»çœ‹å…¶ä»–è§£æäº†ï¼Œå¤§å®¶å¥½å¥‡çš„å¯ä»¥è‡ªå·±å»è¯»

é¦–å…ˆæ¥çœ‹ç¬¬ä¸€æ­¥ï¼Œè¿™ä¸€æ­¥çš„æ ¸å¿ƒä»£ç æ˜¯ï¼š

``` python
analysis_result = await run_research_analyzer(input_source, logger)
```

é‚£æˆ‘ä»¬ç›´æ¥è¿›æ¥çœ‹`run_research_analyzer`

æ‰æ³¨æ„åˆ°`prompt_text`è¿˜æ˜¯`input_source`ï¼Œé‚£è¿™ä¸ªæ˜¯ç±»ä¼¼äºdeep researchçš„agentï¼Ÿæˆ‘ä»¬ç»§ç»­å¾€ä¸‹çœ‹ï¼š

å¥½ï¼Œä¸‹é¢åˆå§‹åŒ–äº†ä¸€ä¸ªæ–°æ™ºèƒ½ä½“ï¼š

``` python
analyzer_agent = Agent(
    name="ResearchAnalyzerAgent",
    instruction=PAPER_INPUT_ANALYZER_PROMPT,
    server_names=get_search_server_names(),
)
```

æˆ‘ä»¬å…ˆæ¥çœ‹Agentç±»ï¼Œ`name`ã€`instruction`ã€`server_names`çš„æ³¨é‡Š

`name` : `Agent name.`

`instruction` : 

```
Instruction for the agent. This can be a string or a callable that takes a dictionary
and returns a string. The callable can be used to generate dynamic instructions based
on the context.
æ™ºèƒ½ä½“çš„æŒ‡ä»¤ã€‚è¿™å¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªå¯è°ƒç”¨å¯¹è±¡ï¼Œè¯¥å¯¹è±¡æ¥æ”¶ä¸€ä¸ªå­—å…¸å¹¶è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚è¯¥å¯è°ƒç”¨å¯¹è±¡å¯ç”¨äºæ ¹æ®ä¸Šä¸‹æ–‡ç”ŸæˆåŠ¨æ€æŒ‡ä»¤ã€‚
```

`server_names` : `List of MCP server names that the agent can access.`ï¼Œå¥¥è¿™ä¸ªå°±æ˜¯mcpæœåŠ¡å™¨çš„é…ç½®ï¼Œé‚£æˆ‘ä»¬å…ˆæ¥å¥½å¥½è¯»è¯»ï¼š`PAPER_INPUT_ANALYZER_PROMPT`

`PAPER_INPUT_ANALYZER_PROMPT` : 

```
You are a precise input analyzer for paper-to-code tasks. You MUST return only a JSON object with no additional text.

Task: Analyze input text and identify file paths/URLs to determine appropriate input type.

Input Analysis Rules:
1. Path Detection:
   - Scan input text for file paths or URLs
   - Use first valid path/URL if multiple found
   - Treat as text input if no valid path/URL found

2. Path Type Classification:
   - URL (starts with http:// or https://): input_type = "url", path = "detected URL"
   - PDF file path: input_type = "file", path = "detected file path"
   - Directory path: input_type = "directory", path = "detected directory path"
   - No path/URL detected: input_type = "text", path = null

3. Requirements Analysis:
   - Extract ONLY requirements from additional_input
   - DO NOT modify or interpret requirements

CRITICAL OUTPUT RESTRICTIONS:
- RETURN ONLY RAW JSON - NO TEXT BEFORE OR AFTER
- NO markdown code blocks (```json)
- NO explanatory text or descriptions
- NO tool call information
- NO analysis summaries
- JUST THE JSON OBJECT BELOW

{
    "input_type": "text|file|directory|url",
    "path": "detected path or URL or null",
    "paper_info": {
        "title": "N/A for text input",
        "authors": ["N/A for text input"],
        "year": "N/A for text input"
    },
    "requirements": [
        "exact requirement from additional_input"
    ]
}
```

ä¸­æ–‡ç‰ˆï¼š

```
ä½ æ˜¯è®ºæ–‡åˆ°ä»£ç ä»»åŠ¡çš„ç²¾å‡†è¾“å…¥åˆ†æå‘˜ã€‚ä½ å¿…é¡»åªè¿”å› JSON å¯¹è±¡ï¼Œä¸åŒ…å«ä»»ä½•é™„åŠ æ–‡æœ¬ã€‚

ä»»åŠ¡ï¼šåˆ†æè¾“å…¥æ–‡æœ¬å¹¶è¯†åˆ«æ–‡ä»¶è·¯å¾„/URLï¼Œä»¥ç¡®å®šåˆé€‚çš„è¾“å…¥ç±»å‹ã€‚

è¾“å…¥åˆ†æè§„åˆ™ï¼š
1. è·¯å¾„æ£€æµ‹ï¼š
- æ‰«æè¾“å…¥æ–‡æœ¬ä¸­çš„æ–‡ä»¶è·¯å¾„æˆ– URL
- å¦‚æœæ‰¾åˆ°å¤šä¸ªï¼Œåˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„è·¯å¾„/URL
- å¦‚æœæœªæ‰¾åˆ°æœ‰æ•ˆçš„è·¯å¾„/URLï¼Œåˆ™è§†ä¸ºæ–‡æœ¬è¾“å…¥

2. è·¯å¾„ç±»å‹åˆ†ç±»ï¼š
- URLï¼ˆä»¥ http:// æˆ– https:// å¼€å¤´ï¼‰ï¼šinput_type = "url", path = "æ£€æµ‹åˆ°çš„ URL"
- PDF æ–‡ä»¶è·¯å¾„ï¼šinput_type = "file", path = "æ£€æµ‹åˆ°çš„æ–‡ä»¶è·¯å¾„"
- ç›®å½•è·¯å¾„ï¼šinput_type = "directory", path = "æ£€æµ‹åˆ°çš„ç›®å½•è·¯å¾„"
- æœªæ£€æµ‹åˆ°è·¯å¾„/URLï¼šinput_type = "text", path = null

3. éœ€æ±‚åˆ†æï¼š
- ä»…ä» additional_input ä¸­æå–éœ€æ±‚
- è¯·å‹¿ä¿®æ”¹æˆ–è§£é‡Šéœ€æ±‚

å…³é”®è¾“å‡ºé™åˆ¶ï¼š
- ä»…è¿”å›åŸå§‹ JSON - å‰åå‡æ— æ–‡æœ¬
- ä¸åŒ…å« Markdown ä»£ç å— (```json)
- ä¸åŒ…å«è§£é‡Šæ€§æ–‡æœ¬æˆ–æè¿°
- ä¸åŒ…å«å·¥å…·è°ƒç”¨ä¿¡æ¯
- ä¸åŒ…å«åˆ†ææ‘˜è¦
- ä»…åŒ…å«ä¸‹é¢çš„ JSON å¯¹è±¡

{
"input_type": "text|file|directory|url",
"path": "æ£€æµ‹åˆ°çš„è·¯å¾„æˆ– URL æˆ– null",
"paper_info": {
"title": "æ–‡æœ¬è¾“å…¥ä¸é€‚ç”¨",
"authors": ["æ–‡æœ¬è¾“å…¥ä¸é€‚ç”¨"],
"year": "æ–‡æœ¬è¾“å…¥ä¸é€‚ç”¨"
},
"requirements": [
"additional_input çš„ç¡®åˆ‡è¦æ±‚"
]
}
```

è¿™ä¸ªå°±æ˜¯åˆ†æinput_sourceçš„ç±»å‹å’Œä¿¡æ¯ï¼Œé‚£ç„æœºå…¶å®åœ¨mcpæœåŠ¡å™¨é‡Œï¼š`get_search_server_names()`ï¼Œæˆ‘ä»¬è¿›æ¥çœ‹çœ‹è¿™ä¸ªï¼Œèƒ½ç”¨å“ªäº›search_mcp

ç„¶åå‘ç°æ˜¯ç”¨äº†ï¼šbraveï¼Œä¹Ÿå°±æ˜¯`@modelcontextprotocol/server-brave-search`ï¼Œæˆ‘ä»¬è¿™é‡Œä¸è¯»è¿™ä¸ªï¼Œå°±æ¥çœ‹çœ‹braveèƒ½å¹²ä»€ä¹ˆã€‚

ç„¶åå‘ç°æ˜¯ä¿¡æ¯æœç´¢ï¼Œè¯»åˆ°è¿™é‡Œæˆ‘è§‰å¾—æŒºå¥‡æ€ªçš„ï¼Œç„¶åæˆ‘å°±åœ¨`agent_orchestration_engine.py`çš„ç¬¬250è¡ŒåŠ äº†å¦‚ä¸‹ä»£ç 

``` python
with open('test.txt', 'w') as f:
    f.write(raw_result)
sys.exit()
```

ç„¶åç”¨`uv run cli/main_cli.py --file test.pdf`è·‘äº†ä¸€ä¸‹ï¼Œæœç„¶è·Ÿæˆ‘æƒ³çš„ä¸€æ ·ï¼Œ`test.txt`é‡Œæ˜¯ï¼š

``` json
{
    "input_type": "file",
    "path": "test.pdf",
    "paper_info": {
        "title": "N/A for text input",
        "authors": ["N/A for text input"],
        "year": "N/A for text input"
    },
    "requirements": []
}
```

æˆ‘æƒ³è¿™ä¸ªpaper_infoæ˜¯æ€ä¹ˆå–å‡ºæ¥çš„ç™¾æ€ä¸å¾—å…¶è§£ï¼Œè·‘äº†ä¸€éå‘ç°ç¡®å®å–ä¸å‡ºæ¥ï¼Œæˆ‘çŒœè¿™é‡Œæ˜¯ä¸ºäº†æå‰ç•™ä¸ªç»“æ„ç»™åé¢ç”¨ï¼Œç»·ï¼Œæ‰€ä»¥è¿™ä¸ªagentçš„æ ¸å¿ƒå…¶å®å°±åœ¨åˆ†å¼€urlå’Œfileï¼Œç„¶ååŒºåˆ†äº†ä¸€ä¸‹æ–‡ä»¶ç±»å‹ï¼Œè¯´å®è¯è¿™ä¸€æ­¥æ„Ÿè§‰å®Œå…¨æ²¡å¿…è¦ï¼Œæ„Ÿè§‰æ˜¯ä¸ºäº†é˜²è ¢ç”¨çš„ï¼ˆ

å¥½æˆ‘ä»¬æ¥ç€å¾€ä¸‹çœ‹ï¼Œä¸‹é¢çš„é‡ç‚¹åœ¨ï¼š

``` python
# Clean LLM output to ensure only pure JSON is returned
try:
    clean_result = extract_clean_json(raw_result)
    print(f"Raw LLM output: {raw_result}")
    print(f"Cleaned JSON output: {clean_result}")
```

è¿™ä¸ª`extract_clean_json`æ˜¯æ‹¿æ¥`Extract clean JSON from LLM output, removing all extra text and formatting.`ï¼Œä¹Ÿå°±æ˜¯ä»å¤§è¯­è¨€æ¨¡å‹è¾“å‡ºä¸­æå–è§„æ•´çš„JSONï¼Œå»é™¤æ‰€æœ‰å¤šä½™çš„æ–‡æœ¬å’Œæ ¼å¼ã€‚

æ¯”å¦‚å¦‚æœå¤§å®¶ç»å¸¸åšllmç›¸å…³åº”ç”¨çš„è¯ï¼Œå¯èƒ½é‡åˆ°è¿‡ä¸æ˜¯çº¯jsonæ–‡æœ¬è¾“å‡ºçš„æƒ…å†µï¼Œæ€ä¹ˆé™åˆ¶éƒ½æ²¡ç”¨ï¼Œç”¨è¿™ä¸ªå‡½æ•°å¯ä»¥ç›´æ¥æå–jsonå‡ºæ¥

å…·ä½“å®ç°æ­¥éª¤æˆ‘å†™åœ¨æ³¨é‡Šé‡Œäº†ï¼Œå¤§å®¶ç›´æ¥çœ‹

```python
def extract_clean_json(llm_output: str) -> str:
    """
    Extract clean JSON from LLM output, removing all extra text and formatting.

    Args:
        llm_output: Raw LLM output

    Returns:
        str: Clean JSON string
    """
    try:
        # é¦–å…ˆå°è¯•å»æ‰ä¸¤ç«¯çš„ç©ºç™½å­—ç¬¦è¡Œä¸è¡Œ
        json.loads(llm_output.strip())
        return llm_output.strip()
    except json.JSONDecodeError:
        pass

    # ç„¶åç”¨æ­£åˆ™è¯•è¯•å–```json ```ä¸­é—´çš„å†…å®¹
    if "```json" in llm_output:
        pattern = r"```json\s*(.*?)\s*```"
        match = re.search(pattern, llm_output, re.DOTALL)
        if match:
            json_text = match.group(1).strip()
            try:
                json.loads(json_text)
                return json_text
            except json.JSONDecodeError:
                pass

    # æ ¹æ®æ‰‹åŠ¨åŒ¹é…èŠ±æ‹¬å·çš„æ–¹æ³•æ¥å–å‡ºjson
    lines = llm_output.split("\n")
    json_lines = []
    in_json = False
    brace_count = 0

    for line in lines:
        stripped = line.strip()
        if not in_json and stripped.startswith("{"):
            in_json = True
            json_lines = [line]
            brace_count = stripped.count("{") - stripped.count("}")
        elif in_json:
            json_lines.append(line)
            brace_count += stripped.count("{") - stripped.count("}")
            if brace_count == 0:
                break

    if json_lines:
        json_text = "\n".join(json_lines).strip()
        try:
            json.loads(json_text)
            return json_text
        except json.JSONDecodeError:
            pass

    # æœ€åå°è¯•ç”¨æ­£åˆ™å–å‡ºjson
    pattern = r"\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}"
    matches = re.findall(pattern, llm_output, re.DOTALL)
    for match in matches:
        try:
            json.loads(match)
            return match
        except json.JSONDecodeError:
            continue

    # If all methods fail, return original output
    return llm_output
```

å¥½äº†ï¼Œæˆ‘ä»¬æŠŠ`run_research_analyzer`çœ‹å®Œäº†ï¼Œå¤§å®¶è®°å¾—æˆ‘ä»¬ä»å“ªæ¥çš„å˜›ï¼Œè®©æˆ‘ä»¬å›åˆ°`orchestrate_research_analysis_agent`

åˆšåˆšçœ‹çš„æ˜¯step 1ï¼Œè®©æˆ‘ä»¬æ¥ä¸‹æ¥çœ‹step 2

æ ¸å¿ƒçš„ä»£ç æ˜¯ï¼š

``` python
download_result = await run_resource_processor(analysis_result, logger)
```

æˆ‘ä»¬è¿›æ¥çœ‹`run_resource_processor`ï¼Œè¿™ä¸ªå‡½æ•°æŒºç®€å•çš„ï¼Œæˆ‘ä»¬æœ‰è¯»ä¹‹å‰çš„agentçš„ç»éªŒï¼Œè¯»è¿™ä¸ªå°±å¾ˆè¿…é€Ÿäº†ï¼Œä¼ å…¥çš„æ˜¯æˆ‘ä»¬ä¹‹å‰ä¸Šä¸€ä¸ªagentçš„åˆ†æç»“æœï¼Œä¸è®°å¾—å¤§å®¶è¿˜è®°ä¸è®°å¾—äº†ï¼Œæ¥çœ‹çœ‹ï¼š

``` json
{
    "input_type": "file",
    "path": "test.pdf",
    "paper_info": {
        "title": "N/A for text input",
        "authors": ["N/A for text input"],
        "year": "N/A for text input"
    },
    "requirements": []
}
```

æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹è¿™ä¸ªagentçš„promptå’Œtools

é¦–å…ˆæ˜¯prompt

```
You are a precise paper downloader that processes input from PaperInputAnalyzerAgent.

Task: Handle paper according to input type and save to "./deepcode_lab/papers/id/id.md"
Note: Generate id (id is a number) by counting files in "./deepcode_lab/papers/" directory and increment by 1.

CRITICAL RULE: NEVER use write_file tool to create paper content directly. Always use file-downloader tools for PDF/document conversion.

Processing Rules:
1. URL Input (input_type = "url"):
   - Use "file-downloader" tool to download paper
   - Extract metadata (title, authors, year)
   - Return saved file path and metadata

2. File Input (input_type = "file"):
   - Move file to "./deepcode_lab/papers/id/" using move_file_to tool
   - The move_file_to tool will automatically convert PDF/documents to .md format
   - NEVER manually extract content or use write_file - let the conversion tools handle this
   - Return new saved file path and metadata

3. Directory Input (input_type = "directory"):
   - Verify directory exists
   - Return to PaperInputAnalyzerAgent for processing
   - Set status as "failure" with message

4. Text Input (input_type = "text"):
   - No file operations needed
   - Set paper_path as null
   - Use paper_info from input

Input Format:
{
    "input_type": "file|directory|url|text",
    "path": "detected path or null",
    "paper_info": {
        "title": "paper title or N/A",
        "authors": ["author names or N/A"],
        "year": "publication year or N/A"
    },
    "requirements": ["requirement1", "requirement2"]
}

Output Format (DO NOT MODIFY):
{
    "status": "success|failure",
    "paper_path": "path to paper file or null for text input",
    "metadata": {
        "title": "extracted or provided title",
        "authors": ["extracted or provided authors"],
        "year": "extracted or provided year"
    }
}
```

ä¸­æ–‡ç¿»è¯‘ï¼š
```
ä½ æ˜¯ä¸€ä¸ªç²¾å‡†çš„è®ºæ–‡ä¸‹è½½å™¨ï¼Œè´Ÿè´£å¤„ç†è¾“å…¥ã€‚

ä»»åŠ¡ï¼šæ ¹æ®è¾“å…¥ç±»å‹å¤„ç†è®ºæ–‡å¹¶ä¿å­˜åˆ°â€œ./deepcode_lab/papers/id/id.mdâ€æ–‡ä»¶ã€‚
æ³¨æ„ï¼šé€šè¿‡ç»Ÿè®¡â€œ./deepcode_lab/papers/â€ç›®å½•ä¸­çš„æ–‡ä»¶æ•°é‡å¹¶åŠ  1 æ¥ç”Ÿæˆ IDï¼ˆid ä¸ºæ•°å­—ï¼‰ã€‚

å…³é”®è§„åˆ™ï¼šåˆ‡å‹¿ä½¿ç”¨ write_file å·¥å…·ç›´æ¥åˆ›å»ºè®ºæ–‡å†…å®¹ã€‚åŠ¡å¿…ä½¿ç”¨æ–‡ä»¶ä¸‹è½½å·¥å…·è¿›è¡Œ PDF/æ–‡æ¡£è½¬æ¢ã€‚

å¤„ç†è§„åˆ™ï¼š
1. URL è¾“å…¥ (input_type = "url")ï¼š
- ä½¿ç”¨â€œfile-downloaderâ€å·¥å…·ä¸‹è½½è®ºæ–‡
- æå–å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€å¹´ä»½ï¼‰
- è¿”å›ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å’Œå…ƒæ•°æ®

2. æ–‡ä»¶è¾“å…¥ (input_type = "file")ï¼š
- ä½¿ç”¨ move_file_to å·¥å…·å°†æ–‡ä»¶ç§»åŠ¨åˆ°â€œ./deepcode_lab/papers/id/â€
- move_file_to å·¥å…·ä¼šè‡ªåŠ¨å°† PDF/æ–‡æ¡£è½¬æ¢ä¸º .md æ ¼å¼
- åˆ‡å‹¿æ‰‹åŠ¨æå–å†…å®¹æˆ–ä½¿ç”¨ write_file - è®©è½¬æ¢å·¥å…·å¤„ç†
- è¿”å›æ–°ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å’Œå…ƒæ•°æ®

3. ç›®å½•è¾“å…¥ (input_type = "directory")ï¼š
- éªŒè¯ç›®å½•æ˜¯å¦å­˜åœ¨
- è¿”å› PaperInputAnalyzerAgent è¿›è¡Œå¤„ç†
- å°†çŠ¶æ€è®¾ç½®ä¸ºâ€œå¤±è´¥â€å¹¶æ˜¾ç¤ºæ¶ˆæ¯

4. æ–‡æœ¬è¾“å…¥ (input_type = "text")ï¼š
- æ— éœ€è¿›è¡Œæ–‡ä»¶æ“ä½œ
- å°† paper_path è®¾ç½®ä¸º null
- ä½¿ç”¨è¾“å…¥ä¸­çš„ paper_info

è¾“å…¥æ ¼å¼ï¼š
{
    "input_type": "file|directory|url|text",
    "path": "detected path or null",
    "paper_info": {
        "title": "paper title or N/A",
        "authors": ["author names or N/A"],
        "year": "publication year or N/A"
    },
    "requirements": ["requirement1", "requirement2"]
}

è¾“å‡ºæ ¼å¼ï¼ˆè¯·å‹¿ä¿®æ”¹ï¼‰ï¼š
{
    "status": "success|failure",
    "paper_path": "path to paper file or null for text input",
    "metadata": {
        "title": "extracted or provided title",
        "authors": ["extracted or provided authors"],
        "year": "extracted or provided year"
    }
}
```

å¯ä»¥çœ‹åˆ°ï¼Œæ ¹æ®`input_type`çš„ä¸åŒè¿›è¡Œä¸åŒçš„æ“ä½œï¼Œç»ˆäºæŠŠ`paper_info`æå–å‡ºæ¥äº†ï¼Œå®³å¾—æˆ‘è¯»äº†å¥½ä¹…ï¼ˆ

æˆ‘ä»¬è¦å…³æ³¨çš„å·¥å…·å…¶å®å°±ä¸¤ä¸ªï¼š`file-downloader`ï¼Œ`move_file_to`ï¼Œæ ¹æ®promptï¼Œå‰è€…ç”¨æ¥ä¸‹è½½è®ºæ–‡ï¼Œåè€…ç”¨æ¥å°†pdfè½¬æ¢æˆ.mdæ ¼å¼

çœ‹ä¸€ä¸‹ä½¿ç”¨çš„å·¥å…·ï¼š`server_names=["filesystem", "file-downloader"]`

``` yaml
filesystem:
  # On windows update the command and arguments to use `node` and the absolute path to the server.
  # Use `npm i -g @modelcontextprotocol/server-filesystem` to install the server globally.
  # Use `npm -g root` to find the global node_modules path.`
  # command: "node"
  # args: ["c:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-filesystem/dist/index.js","."]
  command: "npx"
  args: [ "-y", "@modelcontextprotocol/server-filesystem" ]
file-downloader:
  command: "python"
  args: [ "tools/pdf_downloader.py" ]
  env:
    PYTHONPATH: "."
```

`filesystem`å°±æ˜¯æ–‡ä»¶ç³»ç»Ÿæ“ä½œï¼Œæ²¡å•¥ç‰¹æ®Šçš„ï¼Œæ²¡æœ‰æˆ‘ä»¬æƒ³çœ‹çš„ä¸¤ä¸ªå·¥å…·ï¼Œè®©æˆ‘ä»¬çœ‹`file-downloader`ï¼Œæ¥åˆ°`tools/pdf_downloader.py`

ç›´æ¥æ¥`if __name__ == "__main__":`çœ‹ï¼Œå¯ä»¥å‘ç°æˆ‘ä»¬è¦çœ‹çš„ä¸¤ä¸ªå‡½æ•°éƒ½åœ¨è¿™é‡Œï¼š

``` python
print("\nAvailable tools:")
print(
    "  â€¢ download_files - Download files or move local files from natural language"
)
print("  â€¢ parse_download_urls - Extract URLs, local paths and destination paths")
print("  â€¢ download_file_to - Download a specific file with options")
print("  â€¢ move_file_to - Move a specific local file with options")
print("  â€¢ convert_document_to_markdown - Convert documents to Markdown format")
```

å…¶ä»–çš„å·¥å…·éƒ½æ˜¯ä¸‹è½½ç”¨çš„ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹`move_file_to`ï¼Œåˆšåˆšprompté‡Œæåˆ°äº†å®ƒä¼šæŠŠpdfè½¬æˆmarkdownï¼Œæˆ‘ä»¬æ¥çœ‹ä»–æ˜¯æ€ä¹ˆå¤„ç†çš„

è¦æ‰¾åˆ°è¿™ä¸ªå·¥å…·ç›´æ¥åœ¨æ–‡ä»¶é‡Œæœç´¢`@mcp.tool()`å³å¯ï¼Œå¾ˆå¿«èƒ½å®šä½åˆ°è¯¥å·¥å…·ï¼Œè¿™ä¸ªå·¥å…·çš„æ³¨é‡Šæ€ä¹ˆå…¨æ˜¯ä¸­æ–‡ï¼Œçœ‹æ¥åˆæ˜¯ä¸­å›½å¼€å‘è€…å†™çš„ï¼ˆ

å‰é¢çš„ä»£ç éƒ½æ˜¯åœ¨å¤„ç†è·¯å¾„ï¼Œå’Œåˆ¤æ–­æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œæ ¸å¿ƒä»£ç åœ¨

``` python
# æ‰§è¡Œç§»åŠ¨
result = await move_local_file(source, target_path)
```

æ‰€ä»¥æˆ‘ä»¬è¿›æ¥çœ‹`move_local_file`ï¼Œè¿™ä¸ªå‡½æ•°çš„æ ¸å¿ƒä»£ç åœ¨ï¼š

``` python
# æ‰§è¡Œç§»åŠ¨æ“ä½œ
shutil.move(source_path, destination)
```

å¥½å§å°±æ˜¯æ–‡ä»¶å¤åˆ¶ï¼Œåœ¨`move_local_file`é‡Œç»§ç»­å¾€ä¸‹çœ‹ï¼Œå‘ç°äº†ç§»åŠ¨æˆåŠŸæœ‰ä¸€ä¸ªè½¬æ¢çš„ä»£ç 

``` python
conversion_msg = await perform_document_conversion(
    target_path, extract_images=True
)
```

é‚£æˆ‘ä»¬æ¥çœ‹`perform_document_conversion`ï¼Œå¾ˆå®¹æ˜“å‘ç°å®ƒå…¶å®å°±æ˜¯ç”¨`PyPDF2`åŒ…å»åšäº†ä¸ªè½¬æ¢ï¼Œæ ¸å¿ƒä»£ç åœ¨

``` python
simple_converter = SimplePdfConverter()
conversion_result = simple_converter.convert_pdf_to_markdown(file_path)
```

æˆ‘ä»¬å…ˆçœ‹ä¸€ä¸‹`SimplePdfConverter`ç±»æœ‰æ²¡æœ‰`init`å‡½æ•°ï¼Œå¥½æ²¡æœ‰ï¼Œé‚£æˆ‘ä»¬è¿›æ¥çœ‹`convert_pdf_to_markdown`

å•Šè¿™å®ƒè¿™ä¸ªé€»è¾‘æ²¡æˆ‘æƒ³çš„é«˜å¤§ä¸Šï¼Œå°±æ˜¯æ‹¿`PyPDF2`å¤„ç†é‡Œä¸€ä¸‹ï¼Œæˆ‘æŠŠè¿™ä¸ªå•ç‹¬æå‡ºæ¥å½“ä¸€ä¸ªè„šæœ¬è·‘äº†ä¸€ä¸‹ï¼Œä¸‹é¢æ˜¯æˆ‘æå–å‡ºçš„è„šæœ¬çš„å†…å®¹

``` python
import os
from datetime import datetime
from typing import Dict, Optional, Any

import PyPDF2


def convert_pdf_to_markdown(
        input_file: str, output_file: Optional[str] = None
) -> Dict[str, Any]:
    """
    ä½¿ç”¨PyPDF2å°†PDFè½¬æ¢ä¸ºMarkdownæ ¼å¼

    Args:
        input_file: è¾“å…¥PDFæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºMarkdownæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰

    Returns:
        è½¬æ¢ç»“æœå­—å…¸
    """

    try:
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(input_file):
            return {
                "success": False,
                "error": f"Input file not found: {input_file}",
            }

        # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œè‡ªåŠ¨ç”Ÿæˆ
        if not output_file:
            base_name = os.path.splitext(input_file)[0]
            output_file = f"{base_name}.md"

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_file)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)

        # æ‰§è¡Œè½¬æ¢
        start_time = datetime.now()

        # è¯»å–PDFæ–‡ä»¶
        with open(input_file, "rb") as file:
            pdf_reader = PyPDF2.PdfReader(file)
            text_content = []

            # æå–æ¯é¡µæ–‡æœ¬
            for page_num, page in enumerate(pdf_reader.pages, 1):
                text = page.extract_text()
                if text.strip():
                    text_content.append(f"## Page {page_num}\n\n{text.strip()}\n\n")

        # ç”ŸæˆMarkdownå†…å®¹
        markdown_content = f"# Extracted from {os.path.basename(input_file)}\n\n"
        markdown_content += f"*Total pages: {len(pdf_reader.pages)}*\n\n"
        markdown_content += "---\n\n"
        markdown_content += "".join(text_content)

        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(markdown_content)

        # è®¡ç®—è½¬æ¢æ—¶é—´
        duration = (datetime.now() - start_time).total_seconds()

        # è·å–æ–‡ä»¶å¤§å°
        input_size = os.path.getsize(input_file)
        output_size = os.path.getsize(output_file)

        return {
            "success": True,
            "input_file": input_file,
            "output_file": output_file,
            "input_size": input_size,
            "output_size": output_size,
            "duration": duration,
            "markdown_content": markdown_content,
            "pages_extracted": len(pdf_reader.pages),
        }

    except Exception as e:
        return {
            "success": False,
            "input_file": input_file,
            "error": f"Conversion failed: {str(e)}",
        }


if __name__ == "__main__":
    result = convert_pdf_to_markdown("test.pdf", "test.md")
    print(result)
```

è·‘äº†ä¸€ä¸‹ï¼š
```
{'success': True, 'input_file': 'test.pdf', 'output_file': 'test.md', 'input_size': 1852204, 'output_size': 59825, 'duration': 0.124518, 'pages_extracted': 15}
```

é€Ÿåº¦è¿˜æŒºå¿«ï¼Œä¹Ÿä¸çŸ¥é“å¯¹å›¾ç‰‡åšæ²¡åšå¤„ç†ï¼Œå…¶å®å°±æ˜¯ç”¨`PyPDF2`ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸æ·±å…¥çœ‹äº†ï¼Œé‚£æˆ‘ä»¬å°±æŠŠ`run_resource_processor`çœ‹å®Œäº†ï¼Œè¿™ä¸ªå‡½æ•°è¿”å›çš„å°±æ˜¯llmå“åº”çš„å†…å®¹ï¼Œä¹Ÿå°±æ˜¯ï¼š

```
{
    "status": "success|failure",
    "paper_path": "path to paper file or null for text input",
    "metadata": {
        "title": "extracted or provided title",
        "authors": ["extracted or provided authors"],
        "year": "extracted or provided year"
    }
}
```

ç„¶åå‘ç°`orchestrate_research_analysis_agent`ä¹Ÿè¯»å®Œäº†ï¼ŒçœŸå¥½

Phase 2åˆ°è¿™é‡Œä¹Ÿç»“æŸå•¦ï¼Œæˆ‘ä»¬æ‰“ä¸ªæ–­ç‚¹æ¥çœ‹ä¸‹è¿è¡Œåˆ°è¿™é‡Œçš„ç»“æœ

``` python
print("analysis_result:" + analysis_result)
print("download_result:" + download_result)

sys.exit()
```

æ¥çœ‹çœ‹`analysis_result`

``` json
{
    "input_type": "file",
    "path": "test.pdf",
    "paper_info": {
        "title": "N/A for text input",
        "authors": ["N/A for text input"],
        "year": "N/A for text input"
    },
    "requirements": []
}
```

æ¥çœ‹çœ‹`download_result`

```
I'll process this file input by first checking the papers directory to generate the 
correct ID, then moving the file to the appropriate location.
[Calling tool filesystem_list_directory with args {'path': './deepcode_lab/papers'}]
I need to create the papers directory first, then generate the ID.
[Calling tool filesystem_create_directory with args {'path': './deepcode_lab/papers'}]
[Calling tool filesystem_list_directory with args {'path': './deepcode_lab/papers'}]
The papers directory is empty, so the ID will be 1. Now I'll create the directory for this paper and move 
the file using the file-downloader tool.
[Calling tool filesystem_create_directory with args {'path': './deepcode_lab/papers/1'}]
[Calling tool file-downloader_move_file_to with args {'source': 'test.pdf', 'destination': 
'./deepcode_lab/papers/1/1.md'}]
Perfect! The file has been moved and converted to Markdown. Now let me read the converted file to extract 
metadata.
[Calling tool filesystem_read_text_file with args {'path': './deepcode_lab/papers/1/1.md', 'head': 50}]
Excellent! I can extract the metadata from the converted paper. Let me get a bit more content to ensure I 
have complete information.
[Calling tool filesystem_read_text_file with args {'path': './deepcode_lab/papers/1/1.md', 'head': 20}]
Perfect! I have successfully processed the file input and extracted the metadata. Here's the output:

{
    "status": "success",
    "paper_path": "./deepcode_lab/papers/1/1.md",
    "metadata": {
        "title": "Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained 
Contrastive EEG-Text Masked Autoencoder",
        "authors": ["Jiaqi Wang", "Zhenxi Song", "Zhengyu Ma", "Xipeng Qiu", "Min Zhang", "Zhiguo Zhang"],
        "year": "2024"
    }
}

The file has been successfully moved from `test.pdf` to `./deepcode_lab/papers/1/1.md` and automatically 
converted from PDF to Markdown format. The conversion extracted 15 pages and I was able to extract the 
complete metadata including the title, authors, and publication year from the ACL 2024 conference 
proceedings.
```

æˆ‘æŠŠå·¥å…·è°ƒç”¨ç”»æˆäº†æµç¨‹å›¾

```mermaid
graph TD
    A[filesystem_list_directory<br>path: './deepcode_lab/papers']
    A --> B[filesystem_create_directory<br>path: './deepcode_lab/papers']
    B --> C[filesystem_list_directory<br>path: './deepcode_lab/papers']
    C --> D[filesystem_create_directory<br>path: './deepcode_lab/papers/1']
    D --> E[file-downloader_move_file_to<br>source: 'test.pdf'<br>dest: '.../papers/1/1.md']
    E --> F[filesystem_read_text_file<br>path: '.../papers/1/1.md'<br>head: 50]
    F --> G[filesystem_read_text_file<br>path: '.../papers/1/1.md'<br>head: 20]
```

é‚£ä¹ˆå¾ˆæ¸…æ™°äº†ï¼ŒåŸæ¥meta-dataçš„æå–æœ‰read_text_fileçš„è¿‡ç¨‹ï¼Œé‚£æ²¡ä»€ä¹ˆé—®é¢˜äº†ï¼Œphase2åœ†æ»¡ç»“æŸ

### phase3

è¿™ä¸ªé˜¶æ®µå«Phase 3: Workspace Infrastructure Synthesisï¼Œä½†æ˜¯æˆ‘æ²¡ææ‡‚æ€ä¹ˆç¿»è¯‘ï¼Œå¤§å®¶å§‘ä¸”è®¤ä¸ºæ˜¯è§„åˆ’æ–‡ä»¶è·¯å¾„

æ ¸å¿ƒä»£ç åœ¨ï¼š

``` python
dir_info = await synthesize_workspace_infrastructure_agent(
    download_result, logger, workspace_dir
)
```

è®©æˆ‘ä»¬æ¥çœ‹çœ‹`synthesize_workspace_infrastructure_agent`

é¦–å…ˆæ¥çœ‹è¿™ä¸€æ®µï¼š

``` python
# Parse download result to get file information
result = await FileProcessor.process_file_input(
    download_result, base_dir=workspace_dir
)
```

è¿›æ¥çœ‹çœ‹`FileProcessor.process_file_input`

è¿™ä¸ªå‡½æ•°å¤ªå¤æ‚äº†ï¼Œä½†æˆ‘æå–å‡ºæ¥è·‘äº†ä¸€ä¸‹å‘ç°æ˜¯è·å–è®ºæ–‡çš„åœ°å€ï¼Œæ¯”å¦‚æˆ‘è¿™é‡Œè·‘å‡ºæ¥å°±æ˜¯ï¼š`/Users/dinglizhi/Desktop/coderead/DeepCode/deepcode_lab/papers/1`

è¿™ä¸ªå†™æ³•å®Œå…¨æ²¡å¿…è¦å•Šï¼Œç›´æ¥æå–`download_result`å°±å¥½äº†ï¼Œæˆ‘ä»¬æœ‰`extract_clean_json`è¿˜è®°å¾—å—ï¼Œèƒ½å–å‡ºæ¥`"paper_path": "./deepcode_lab/papers/1/1.md",`ï¼Œå°±è§£å†³äº†ï¼Œæ‰€ä»¥è¿™ç§é«˜starçš„å¼€æºé¡¹ç›®å†™æ³•ä¹Ÿå¹¶éå®Œç¾ï¼Œå¤§å®¶è¯»çš„æ—¶å€™ä¸€å®šè¦æœ‰è‡ªå·±çš„æ€è€ƒ

ç„¶åè¿™ä¸ªå‡½æ•°è¿”å›çš„å…¶å®æ˜¯æœ€ä½³æ„æˆï¼š

``` python
return {
    "paper_dir": paper_dir,
    "standardized_text": result["standardized_text"],
    "reference_path": os.path.join(paper_dir, "reference.txt"),
    "initial_plan_path": os.path.join(paper_dir, "initial_plan.txt"),
    "download_path": os.path.join(paper_dir, "github_download.txt"),
    "index_report_path": os.path.join(paper_dir, "codebase_index_report.txt"),
    "implementation_report_path": os.path.join(
        paper_dir, "code_implementation_report.txt"
    ),
    "workspace_dir": workspace_dir,
}
```

### phase3.5

è¯¥é˜¶æ®µæ˜¯åšæ–‡æ¡£çš„åˆ†å‰²å’Œé¢„å¤„ç†

ä¸‹é¢æ˜¯æ ¸å¿ƒä»£ç ï¼š

``` python
segmentation_result = await orchestrate_document_preprocessing_agent(
    dir_info, logger
)
```

æ‰€ä»¥è®©æˆ‘ä»¬è¿›æ¥çœ‹`orchestrate_document_preprocessing_agent`

å…ˆçœ‹æ³¨é‡Šï¼š

```
Orchestrate adaptive document preprocessing with intelligent segmentation control.

This agent autonomously determines whether to use document segmentation based on
configuration settings and document size, then applies the appropriate processing strategy.
```

ä¸­æ–‡ï¼š

```
é€šè¿‡æ™ºèƒ½åˆ†å‰²æ§åˆ¶æ¥åè°ƒè‡ªé€‚åº”æ–‡æ¡£é¢„å¤„ç†ã€‚

è¯¥ä»£ç†ä¼šæ ¹æ®é…ç½®è®¾ç½®å’Œæ–‡æ¡£å¤§å°è‡ªä¸»ç¡®å®šæ˜¯å¦ä½¿ç”¨æ–‡æ¡£åˆ†å‰²ï¼Œç„¶ååº”ç”¨é€‚å½“çš„å¤„ç†ç­–ç•¥ã€‚
```

#### step1

ç¬¬ä¸€æ­¥ä¸»è¦æ˜¯æå–æ‰€æœ‰æ–‡ä»¶ï¼Œä»£ç å¾ˆç®€å•ï¼Œå°±ä¸è¿‡å¤šè§£é‡Šäº†ï¼Œæ ¸å¿ƒåœ¨ï¼š

``` python
md_files = [
    f for f in os.listdir(dir_info["paper_dir"]) if f.endswith(".md")
]
```

#### step2

è¯»å–æ–‡ä»¶çš„å†…å®¹æ¥ç¡®å®šæ–‡ä»¶çš„å¤§å°ï¼Œè¿™é‡Œåªè¯»å–äº†ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼Œç„¶ååˆ¤æ–­äº†å¦‚æœè¿˜æ˜¯pdfå°±æ‰“æ–­è®©ä»–å›å»åšæ ¼å¼è½¬æ¢

`document_content`æ˜¯æ–‡ä»¶çš„å†…å®¹

#### step3

ç¡®å®šæ˜¯å¦åˆ†å—

æ ¸å¿ƒä»£ç åœ¨ï¼š

``` python
# Step 3: Determine if segmentation should be used
should_segment, reason = should_use_document_segmentation(document_content)
```

æ‰€ä»¥è®©æˆ‘ä»¬è¿›æ¥çœ‹`should_use_document_segmentation`

è¿™é‡Œä¸»è¦æ˜¯ä¾é `mcp_agent.config.yaml`ä¸­çš„å†…å®¹è¿›è¡Œåˆ¤æ–­ï¼Œä¹Ÿå°±æ˜¯

``` yaml
document_segmentation:
  enabled: true
  size_threshold_chars: 50000
```

å½“enabledçš„æ—¶å€™ï¼Œå¹¶ä¸”æ–‡æ¡£çš„é•¿åº¦ > `size_threshold_chars`çš„æ—¶å€™å°±åˆ†å‰²

ç„¶åæ˜¯æ­£å¼çš„åˆ†å‰²çš„ä»£ç 

``` python
# Prepare document segments using the segmentation agent
segmentation_result = await prepare_document_segments(
    paper_dir=dir_info["paper_dir"], logger=logger
)
```

æ‰€ä»¥æˆ‘ä»¬è¿›æ¥çœ‹`prepare_document_segments`ï¼Œè€æ ·å­å…ˆçœ‹æ³¨é‡Šï¼š

```
Prepare intelligent document segments optimized for planning agents.

This enhanced function leverages semantic analysis to create segments that:
- Preserve algorithm and formula integrity
- Optimize for ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent
- Use adaptive character limits based on content complexity
- Maintain technical content completeness

Called from the orchestration engine (Phase 3.5) to prepare documents
before the planning phase with superior segmentation quality.
```

ä¸­æ–‡ï¼š

```
ä¸ºè§„åˆ’ä»£ç†å‡†å¤‡ä¼˜åŒ–çš„æ™ºèƒ½æ–‡æ¡£åˆ†æ®µã€‚

æ­¤å¢å¼ºåŠŸèƒ½åˆ©ç”¨è¯­ä¹‰åˆ†ææ¥åˆ›å»ºåˆ†æ®µï¼Œå…¶ç‰¹ç‚¹å¦‚ä¸‹ï¼š
- ä¿ç•™ç®—æ³•å’Œå…¬å¼çš„å®Œæ•´æ€§
- é’ˆå¯¹ ConceptAnalysisAgentã€AlgorithmAnalysisAgent å’Œ CodePlannerAgent è¿›è¡Œä¼˜åŒ–
- æ ¹æ®å†…å®¹å¤æ‚åº¦ä½¿ç”¨è‡ªé€‚åº”å­—ç¬¦é™åˆ¶
- ä¿æŒæŠ€æœ¯å†…å®¹çš„å®Œæ•´æ€§

ä»ç¼–æ’å¼•æ“ï¼ˆç¬¬ 3.5 é˜¶æ®µï¼‰è°ƒç”¨ï¼Œç”¨äºåœ¨è§„åˆ’é˜¶æ®µä¹‹å‰å‡†å¤‡æ–‡æ¡£ï¼Œå¹¶å®ç°å“è¶Šçš„åˆ†æ®µè´¨é‡ã€‚
```

è¿™é‡Œçš„æ ¸å¿ƒä»£ç æ˜¯ï¼š

``` python
# Run analysis
result = await run_document_segmentation_analysis(
    paper_dir=paper_dir,
    logger=logger,
    force_refresh=False,  # Use cached analysis if available
)
```

æ‰€ä»¥æˆ‘ä»¬æ¥çœ‹`run_document_segmentation_analysis`

è¿™é‡Œçš„æ ¸å¿ƒä»£ç æ˜¯ï¼š

``` python
# Analyze and prepare document
analysis_result = await agent.analyze_and_prepare_document(
    paper_dir, force_refresh=force_refresh
)
```

æˆ‘ä»¬æ¥çœ‹`analyze_and_prepare_document`

é¦–å…ˆè¿˜æ˜¯å–å‡ºæ‰€æœ‰markdown

ç„¶åæ˜¯åŠ¨æ€ç”Ÿæˆäº†ä¸€æ®µæç¤ºè¯ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹ï¼Œè¿™ä¸ªåº”è¯¥å°±æ˜¯åˆ†å—çš„æ ¸å¿ƒæ‰€åœ¨äº†ï¼š

``` markdown
Please perform intelligent semantic analysis and segmentation for the document in directory: {paper_dir}

Use the analyze_and_segment_document tool with these parameters:
- paper_dir: {paper_dir}
- force_refresh: {force_refresh}

**Focus on these enhanced objectives**:
1. **Semantic Document Classification**: Identify document type using content semantics (research_paper, algorithm_focused, technical_doc, etc.)
2. **Intelligent Segmentation Strategy**: Select the optimal strategy based on content analysis:
   - `semantic_research_focused` for research papers with high algorithm density
   - `algorithm_preserve_integrity` for algorithm-heavy documents
   - `concept_implementation_hybrid` for mixed concept/implementation content
3. **Algorithm Completeness**: Ensure algorithm blocks, formulas, and related descriptions remain logically connected
4. **Planning Agent Optimization**: Create segments that maximize effectiveness for ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent

After segmentation, get a document overview and provide:
- Quality assessment of semantic segmentation approach
- Algorithm/formula integrity verification
- Recommendations for planning agent optimization
- Technical content completeness evaluation
```

ç¿»è¯‘å¦‚ä¸‹ï¼š

``` markdown
è¯·å¯¹ç›®å½• {paper_dir} ä¸­çš„æ–‡æ¡£è¿›è¡Œæ™ºèƒ½è¯­ä¹‰åˆ†æå’Œåˆ†æ®µã€‚

ä½¿ç”¨ analyze_and_segment_document å·¥å…·å¹¶è®¾ç½®ä»¥ä¸‹å‚æ•°ï¼š
- paper_dir: {paper_dir}
- force_refresh: {force_refresh}

**é‡ç‚¹å…³æ³¨ä»¥ä¸‹å¢å¼ºç›®æ ‡**ï¼š
1. **è¯­ä¹‰æ–‡æ¡£åˆ†ç±»**ï¼šä½¿ç”¨å†…å®¹è¯­ä¹‰è¯†åˆ«æ–‡æ¡£ç±»å‹ï¼ˆç ”ç©¶è®ºæ–‡ã€ç®—æ³•é‡ç‚¹ã€æŠ€æœ¯æ–‡æ¡£ç­‰ï¼‰ã€‚
2. **æ™ºèƒ½åˆ†æ®µç­–ç•¥**ï¼šæ ¹æ®å†…å®¹åˆ†æé€‰æ‹©æœ€ä½³ç­–ç•¥ï¼š
- `semantic_research_focused` ç”¨äºç®—æ³•å¯†é›†å‹ç ”ç©¶è®ºæ–‡
- `algorithm_preserve_integrity` ç”¨äºç®—æ³•å¯†é›†å‹æ–‡æ¡£
- `concept_implementation_hybrid` ç”¨äºæ¦‚å¿µ/å®ç°æ··åˆå†…å®¹
3. **ç®—æ³•å®Œæ•´æ€§**ï¼šç¡®ä¿ç®—æ³•å—ã€å…¬å¼å’Œç›¸å…³æè¿°ä¿æŒé€»è¾‘å…³è”
4. **è§„åˆ’ä»£ç†ä¼˜åŒ–**ï¼šåˆ›å»ºå¯æœ€å¤§é™åº¦æé«˜æ•ˆç‡çš„åˆ†æ®µä¸ºäº†ConceptAnalysisAgentã€AlgorithmAnalysisAgent å’Œ CodePlannerAgent

åˆ†å‰²åï¼Œè·å–æ–‡æ¡£æ¦‚è§ˆå¹¶æä¾›ï¼š
- è¯­ä¹‰åˆ†å‰²æ–¹æ³•çš„è´¨é‡è¯„ä¼°
- ç®—æ³•/å…¬å¼çš„å®Œæ•´æ€§éªŒè¯
- è§„åˆ’ä»£ç†ä¼˜åŒ–å»ºè®®
- æŠ€æœ¯å†…å®¹å®Œæ•´æ€§è¯„ä¼°
```

è¿™é‡Œå‘ç°ä»–ç›´æ¥ç”¨äº†llmï¼Œæ‰€ä»¥æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹`__init__`å’Œ`initialize`ï¼Œå‘ç°agentæ˜¯åœ¨`initialize`é‡Œåˆå§‹åŒ–çš„

è¿™ä¸ªagentä¹Ÿæœ‰ä¸€ä¸ªpromptï¼š

``` markdown
You are an intelligent document segmentation coordinator that leverages advanced semantic analysis for optimal document processing.

Your enhanced capabilities include:
1. **Semantic Content Analysis**: Coordinate intelligent document type classification based on content semantics rather than structural patterns
2. **Algorithm Integrity Protection**: Ensure algorithm blocks, formulas, and related content maintain logical coherence
3. **Adaptive Segmentation Strategy**: Select optimal segmentation approaches (semantic_research_focused, algorithm_preserve_integrity, concept_implementation_hybrid, etc.)
4. **Quality Intelligence Validation**: Assess segmentation quality using enhanced metrics for completeness, relevance, and technical accuracy
5. **Planning Agent Optimization**: Ensure segments are specifically optimized for ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent needs

**Key Principles**:
- Prioritize content semantics over mechanical structure
- Preserve algorithm and formula completeness
- Optimize for downstream agent token efficiency
- Ensure technical content integrity
- Provide actionable quality assessments

Use the enhanced document-segmentation tools to deliver superior segmentation results that significantly improve planning agent performance.
```

æ¥çœ‹çœ‹ä¸­æ–‡ç¿»è¯‘ï¼š

``` markdown
æ‚¨æ˜¯ä¸€ä½æ™ºèƒ½æ–‡æ¡£åˆ†å‰²åè°ƒè€…ï¼Œèƒ½å¤Ÿåˆ©ç”¨é«˜çº§è¯­ä¹‰åˆ†æå®ç°æœ€ä½³æ–‡æ¡£å¤„ç†ã€‚

æ‚¨çš„å¢å¼ºåŠŸèƒ½åŒ…æ‹¬ï¼š
1. **è¯­ä¹‰å†…å®¹åˆ†æ**ï¼šåŸºäºå†…å®¹è¯­ä¹‰è€Œéç»“æ„æ¨¡å¼åè°ƒæ™ºèƒ½æ–‡æ¡£ç±»å‹åˆ†ç±»
2. **ç®—æ³•å®Œæ•´æ€§ä¿æŠ¤**ï¼šç¡®ä¿ç®—æ³•å—ã€å…¬å¼å’Œç›¸å…³å†…å®¹ä¿æŒé€»è¾‘ä¸€è‡´æ€§
3. **è‡ªé€‚åº”åˆ†æ®µç­–ç•¥**ï¼šé€‰æ‹©æœ€ä½³åˆ†æ®µæ–¹æ³•ï¼ˆä»¥è¯­ä¹‰ç ”ç©¶ä¸ºä¸­å¿ƒã€ç®—æ³•ä¿ç•™å®Œæ•´æ€§ã€æ¦‚å¿µå®ç°æ··åˆç­‰ï¼‰
4. **è´¨é‡æ™ºèƒ½éªŒè¯**ï¼šä½¿ç”¨å¢å¼ºçš„å®Œæ•´æ€§ã€ç›¸å…³æ€§å’ŒæŠ€æœ¯å‡†ç¡®æ€§æŒ‡æ ‡è¯„ä¼°åˆ†æ®µè´¨é‡
5. **è§„åˆ’ä»£ç†ä¼˜åŒ–**ï¼šç¡®ä¿åˆ†æ®µé’ˆå¯¹ ConceptAnalysisAgentã€AlgorithmAnalysisAgent å’Œ CodePlannerAgent çš„éœ€æ±‚è¿›è¡Œä¸“é—¨ä¼˜åŒ–

**å…³é”®åŸåˆ™**ï¼š
- ä¼˜å…ˆè€ƒè™‘å†…å®¹è¯­ä¹‰è€Œéæœºæ¢°ç»“æ„
- ä¿æŒç®—æ³•å’Œå…¬å¼çš„å®Œæ•´æ€§
- ä¼˜åŒ–ä¸‹æ¸¸ä»£ç†ä»¤ç‰Œæ•ˆç‡
- ç¡®ä¿æŠ€æœ¯å†…å®¹å®Œæ•´æ€§
- æä¾›å¯æ“ä½œçš„è´¨é‡è¯„ä¼°

ä½¿ç”¨å¢å¼ºçš„æ–‡æ¡£åˆ†å‰²å·¥å…·å¯æä¾›å“è¶Šçš„åˆ†å‰²ç»“æœï¼Œä»è€Œæ˜¾è‘—æé«˜è§„åˆ’ä»£ç†çš„æ€§èƒ½ã€‚
```

mcpæœ‰ä¸¤ä¸ªï¼š`document-segmentation`å’Œ`filesystem`ï¼Œåˆšåˆšçš„messageé‡Œæåˆ°äº†å·¥å…·ï¼š`analyze_and_segment_document`åº”è¯¥åœ¨`document-segmentation`é‡Œï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹`document-segmentation`

``` yaml
document-segmentation:
  args:
    - tools/document_segmentation_server.py
  command: python
  description: Document segmentation server - Provides intelligent document analysis
    and segmented reading to optimize token usage
  env:
    PYTHONPATH: .
```

æ‰€ä»¥æˆ‘ä»¬æ¥çœ‹`tools/document_segmentation_server.py`ï¼Œæ–‡ä»¶çš„æœ€ä¸Šé¢æœ‰`analyze_and_segment_document`çš„æ³¨é‡Š

``` markdown
ğŸ“„ analyze_and_segment_document(paper_dir: str, force_refresh: bool = False)
   Purpose: Analyzes document structure and creates intelligent segments
   - Detects document type (research paper, technical doc, algorithm-focused, etc.)
   - Selects optimal segmentation strategy based on content analysis
   - Creates semantic segments preserving algorithm and concept integrity
   - Stores segmentation index for efficient retrieval
   - Returns: JSON with segmentation status, strategy used, and segment count
```

çœ‹çœ‹ä¸­æ–‡ï¼š

```
ğŸ“„ analyze_and_segment_document(paper_dir: str, force_refresh: bool = False)
ç”¨é€”ï¼šåˆ†ææ–‡æ¡£ç»“æ„å¹¶åˆ›å»ºæ™ºèƒ½åˆ†æ®µ
- æ£€æµ‹æ–‡æ¡£ç±»å‹ï¼ˆç ”ç©¶è®ºæ–‡ã€æŠ€æœ¯æ–‡æ¡£ã€ç®—æ³•ç±»æ–‡æ¡£ç­‰ï¼‰
- æ ¹æ®å†…å®¹åˆ†æé€‰æ‹©æœ€ä½³åˆ†æ®µç­–ç•¥
- åˆ›å»ºè¯­ä¹‰åˆ†æ®µï¼Œä¿ç•™ç®—æ³•å’Œæ¦‚å¿µçš„å®Œæ•´æ€§
- å­˜å‚¨åˆ†æ®µç´¢å¼•ä»¥ä¾¿é«˜æ•ˆæ£€ç´¢
- è¿”å›ï¼šåŒ…å«åˆ†æ®µçŠ¶æ€ã€æ‰€ç”¨ç­–ç•¥å’Œåˆ†æ®µè®¡æ•°çš„ JSON æ•°æ®
```

è€æ ·å­ï¼Œæ¥æœ`@mcp.tool()`ï¼Œæ‰¾åˆ°`analyze_and_segment_document`å®ç°çš„åœ°æ–¹ï¼Œæ¥ä»ä¸Šå¾€ä¸‹çœ‹ï¼Œé¦–å…ˆè¿˜æ˜¯ç»å…¸å–å‡ºæ‰€æœ‰æ–‡ä»¶ï¼Œå¾ˆå¥‡æ€ªè¿™è¾¹éƒ½åªä¼šå¤„ç†ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼Œé‚£å¤šæ–‡ä»¶çš„æ„ä¹‰åœ¨å“ªé‡Œå‘¢ï¼Œæä¸æ‡‚

é¦–å…ˆæ˜¯å¦‚æœè€çš„é…ç½®æ–‡ä»¶å­˜åœ¨å°±åŠ è½½ï¼Œä¸å­˜åœ¨å°±è¯»å…¥è¦å¤„ç†çš„æ–‡ä»¶

é¦–å…ˆæ˜¯åˆ†ææ–‡ä»¶éƒ¨åˆ†ï¼š

``` python
# Analyze document
analyzer = DocumentAnalyzer()
doc_type, confidence = analyzer.analyze_document_type(content)
strategy = analyzer.detect_segmentation_strategy(content, doc_type)
```

`DocumentAnalyzer`ç±»ä¸­æ²¡æœ‰åˆå§‹åŒ–å‡½æ•°ï¼Œé‚£ä¹ˆæˆ‘ä»¬ç›´æ¥å¾€ä¸‹çœ‹`analyze_document_type`ï¼Œä»–é¦–å…ˆæŠŠå†…å®¹å°å†™äº†ï¼Œç„¶åå¼€å§‹è®¡ç®—æƒé‡ï¼Ÿ

æˆ‘ä»¬æ¥çœ‹çœ‹`_calculate_weighted_score`ï¼Œå¤§å®¶çœ‹äº†ä»¥åè‡ªè¡Œå¾€ä¸Šç¿»çœ‹çœ‹å°±çŸ¥é“äº†ï¼Œå…¶å®å°±æ˜¯åˆ¤æ–­è¯æ±‡å‡ºç°çš„æ¬¡æ•°æ¥è®¡ç®—æƒé‡

ç„¶åä¸‹é¢æœ‰ä¸€ä¸ªDetect semantic patterns of document typesï¼Œä¹Ÿå°±æ˜¯è¯­ä¹‰åˆ†ç±»ï¼Œæ¥çœ‹ä¸‹`_detect_pattern_score`ï¼Œè¿™ä¸ªå’Œ`_calculate_weighted_score`ç±»ä¼¼ï¼Œæˆ‘ä»¬å°±ä¸çœ‹äº†ï¼Œç„¶åä¸‹é¢å°±æ˜¯ç®—æ€»å¾—åˆ†ï¼Œç»™å‡ºåˆ†ç±»å’Œç½®ä¿¡åº¦

ç„¶åæ¥çœ‹ä¸€ä¸‹`detect_segmentation_strategy`ï¼Œè¿™é‡Œè¿˜æ˜¯å„ç§å…³é”®è¯æ£€ç´¢ï¼Œç„¶åç¡®å®šæ–‡æ¡£ç±»å‹

åœ¨å·¥å…·é‡Œç»§ç»­å¾€ä¸‹çœ‹ï¼Œåˆ›å»ºåˆ†å—çš„å…·ä½“é€»è¾‘ï¼š

``` python
# Create segments
segments = segmenter.segment_document(content, strategy)
```

æ¥çœ‹`segment_document`

``` python
def segment_document(self, content: str, strategy: str) -> List[DocumentSegment]:
    """
    Perform intelligent segmentation using the specified strategy
    """
    if strategy == "semantic_research_focused":
        return self._segment_research_paper_semantically(content)
    elif strategy == "algorithm_preserve_integrity":
        return self._segment_preserve_algorithm_integrity(content)
    elif strategy == "concept_implementation_hybrid":
        return self._segment_concept_implementation_hybrid(content)
    elif strategy == "semantic_chunking_enhanced":
        return self._segment_by_enhanced_semantic_chunks(content)
    elif strategy == "content_aware_segmentation":
        return self._segment_content_aware(content)
    else:
        # Compatibility with legacy strategies
        return self._segment_by_enhanced_semantic_chunks(content)
```

è¿™ä¸ªå¾ˆæœ´ç´ äº†ï¼Œå¤§å®¶è‡ªå·±çœ‹å§ï¼Œæˆ‘ä»¬æƒå½“åˆ†å—è·‘å®Œäº†

è€æ ·å­ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªphaseç»“æŸæ‰“ä¸ªæ–­ç‚¹ï¼Œçœ‹çœ‹è¿è¡Œåˆ°è¿™é‡Œå‘ç”Ÿäº†ä»€ä¹ˆ

ç»“æœæ¯›éƒ½æ²¡ç»™æˆ‘åˆ†ç‰‡å‡ºæ¥ï¼Œçœ‹çœ‹`document_index.json`

``` json
{
  "document_path": "/Users/dinglizhi/Desktop/coderead/DeepCode/deepcode_lab/papers/1/1.md",
  "document_type": "research_paper",
  "segmentation_strategy": "concept_implementation_hybrid",
  "total_segments": 0,
  "total_chars": 59608,
  "segments": [],
  "created_at": "2025-10-11T15:04:27.466987"
}
```

æˆ‘ä»¬å°±å½“ä»–æ²¡åˆ†ç‰‡è¿™ä¸ªåŠŸèƒ½å¥½å§

### phase4

å¥½çš„å¥½çš„ï¼Œç»ˆäºå®Œæˆäº†å¤§éƒ¨åˆ†å‰ç½®å·¥ä½œï¼Œç°åœ¨æ­£å¼è¿›å…¥ä»£ç è§„åˆ’çš„æ™ºèƒ½ä½“

å’±ä»¬è¿›æ¥çœ‹`await orchestrate_code_planning_agent(dir_info, logger, progress_callback)`

è€æ ·å­å…ˆæ¥çœ‹ä¸‹æ³¨é‡Šï¼š

```
Orchestrate intelligent code planning with automated design analysis.

This agent autonomously generates optimal code reproduction plans and implementation strategies using AI-driven code analysis and planning principles.
```

ç¿»è¯‘ï¼š

```
é€šè¿‡è‡ªåŠ¨åŒ–è®¾è®¡åˆ†æï¼Œç»Ÿç­¹æ™ºèƒ½åŒ–ä»£ç è§„åˆ’ã€‚

è¯¥æ™ºèƒ½ä½“åˆ©ç”¨ AI é©±åŠ¨çš„ä»£ç åˆ†æä¸è§„åˆ’åŸç†ï¼Œè‡ªä¸»ç”Ÿæˆæœ€ä¼˜çš„ä»£ç å¤ç°æ–¹æ¡ˆåŠå®æ–½ç­–ç•¥ã€‚
```

å¥½ï¼Œè¿™é‡Œé¦–å…ˆæ ‡è®°ä¸Šplanæ–‡ä»¶ï¼Œå¦‚æœè¿™ä¸ªæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå»è·‘ä¸€ä¸ªå«`run_code_analyzer`çš„å‡½æ•°ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹è¿™ä¸ª

è¿™ä¸ªå‡½æ•°çš„æ³¨é‡Šå¦‚ä¸‹ï¼š

```
Run the adaptive code analysis workflow using multiple agents for comprehensive code planning.

This function orchestrates three specialized agents with adaptive configuration:
- ConceptAnalysisAgent: Analyzes system architecture and conceptual framework
- AlgorithmAnalysisAgent: Extracts algorithms, formulas, and technical details
- CodePlannerAgent: Integrates outputs into a comprehensive implementation plan
```

ç¿»è¯‘ï¼š

```
è¿è¡Œè‡ªé€‚åº”ä»£ç åˆ†æå·¥ä½œæµï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“ååŒè¿›è¡Œå…¨é¢çš„ä»£ç è§„åˆ’ã€‚

è¯¥å‡½æ•°é€šè¿‡è‡ªé€‚åº”é…ç½®åè°ƒä¸‰ä¸ªä¸“ä¸šæ™ºèƒ½ä½“ï¼š

ConceptAnalysisAgentï¼ˆæ¦‚å¿µåˆ†ææ™ºèƒ½ä½“ï¼‰ï¼šåˆ†æç³»ç»Ÿæ¶æ„ä¸æ¦‚å¿µæ¡†æ¶ã€‚
AlgorithmAnalysisAgentï¼ˆç®—æ³•åˆ†ææ™ºèƒ½ä½“ï¼‰ï¼šæå–ç®—æ³•ã€å…¬å¼åŠæŠ€æœ¯ç»†èŠ‚ã€‚
CodePlannerAgentï¼ˆä»£ç è§„åˆ’æ™ºèƒ½ä½“ï¼‰ï¼šå°†è¾“å‡ºå†…å®¹æ•´åˆä¸ºå…¨é¢çš„å®ç°æ–¹æ¡ˆã€‚
```

é¦–å…ˆè¿™é‡Œé…ç½®äº†ä¸‰ä¸ªæ™ºèƒ½ä½“ï¼Œå…ˆè·å–å¿…è¦çš„é…ç½®

æˆ‘ä»¬ä¸€ä¸ªä¸ªçœ‹

``` python
search_server_names = get_search_server_names()
agent_config = get_adaptive_agent_config(use_segmentation, search_server_names)
prompts = get_adaptive_prompts(use_segmentation)
```

ç¬¬ä¸€ä¸ªé…ç½®ï¼Œå°±æ˜¯å»searchç”¨çš„mcpï¼Œé»˜è®¤çš„å°±æ˜¯braveï¼Œè¿™ä¸ªæ²¡ä»€ä¹ˆå¥½åœ¨æ„çš„

ç¬¬äºŒä¸ªå»é…ç½®ä¸åŒçš„agentï¼Œæˆ‘ä»¬è¿›æ¥çœ‹`get_adaptive_agent_config`

è¯¶è¿™é‡Œå¾ˆå¥‡æ€ªï¼Œ

``` python
config = {
    "concept_analysis": [],
    "algorithm_analysis": search_server_names.copy(),
    "code_planner": search_server_names.copy(),
}
```

åŸºç¡€è®¾ç½®æ˜¯ï¼Œ`concept_analysis`æ˜¯ç©ºï¼Œ`algorithm_analysis`å’Œ`code_planner`éƒ½æœ‰æœç´¢mcp

ç„¶åç»™è¿™ä¸‰ä¸ªagentéƒ½åˆ†é…äº†`document-segmentation`ä¹Ÿå°±æ˜¯åˆ†ç‰‡ç”¨çš„mcpï¼Œé‡Œé¢æœ‰`read_document_segments`å’Œ`get_document_overview`ä¸¤ä¸ªå·¥å…·åº”è¯¥æ˜¯èƒ½ç”¨åˆ°çš„

ç„¶åå°±æ˜¯è·å–é€‚é…çš„æç¤ºè¯ï¼š`get_adaptive_prompts`

æœ€ç»ˆè·å–åˆ°çš„æç¤ºè¯å¦‚ä¸‹ï¼š

``` python
return {
    "concept_analysis": PAPER_CONCEPT_ANALYSIS_PROMPT,
    "algorithm_analysis": PAPER_ALGORITHM_ANALYSIS_PROMPT,
    "code_planning": CODE_PLANNING_PROMPT,
}
```

æˆ‘ä»¬å…ˆæŠŠæ‰§è¡Œçš„æµç¨‹çœ‹ä¸€ä¸‹å†çœ‹æ¯ä¸€ä¸ªæç¤ºè¯ï¼Œå®šä¹‰äº†ä¸€ä¸ªå¤§çš„agent

``` python
code_aggregator_agent = ParallelLLM(
    fan_in_agent=code_planner_agent,
    fan_out_agents=[concept_analysis_agent, algorithm_analysis_agent],
    llm_factory=get_preferred_llm_class(),
)
```

çœ‹èµ·æ¥åƒæ˜¯æŠŠ`code_planner_agent`çš„ç»“æœä¼ ç»™`concept_analysis_agent`å’Œ`algorithm_analysis_agent`ï¼Œå†æ±‡æ€»ç»“æœï¼Œæ€»ä¹‹æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹`ParallelLLM`

```
LLMs can sometimes work simultaneously on a task (fan-out)
and have their outputs aggregated programmatically (fan-in).
This workflow performs both the fan-out and fan-in operations using  LLMs.
From the user's perspective, an input is specified and the output is returned.

When to use this workflow:
    Parallelization is effective when the divided subtasks can be parallelized
    for speed (sectioning), or when multiple perspectives or attempts are needed for
    higher confidence results (voting).

Examples:
    Sectioning:
        - Implementing guardrails where one model instance processes user queries
        while another screens them for inappropriate content or requests.

        - Automating evals for evaluating LLM performance, where each LLM call
        evaluates a different aspect of the modelâ€™s performance on a given prompt.

    Voting:
        - Reviewing a piece of code for vulnerabilities, where several different
        agents review and flag the code if they find a problem.

        - Evaluating whether a given piece of content is inappropriate,
        with multiple agents evaluating different aspects or requiring different
        vote thresholds to balance false positives and negatives.
```

ç¿»è¯‘ä¸ºï¼š

```
LLM æœ‰æ—¶å¯ä»¥åŒæ—¶å¤„ç†å¤šé¡¹ä»»åŠ¡ï¼ˆæ‰‡å‡º/fan-outï¼‰ï¼Œå¹¶ä»¥ç¼–ç¨‹æ–¹å¼æ±‡æ€»å…¶è¾“å‡ºç»“æœï¼ˆæ‰‡å…¥/fan-inï¼‰ã€‚æ­¤å·¥ä½œæµåˆ©ç”¨ LLM åŒæ—¶æ‰§è¡Œæ‰‡å‡ºå’Œæ‰‡å…¥æ“ä½œã€‚ä»ç”¨æˆ·çš„è§’åº¦æ¥çœ‹ï¼Œåªéœ€æŒ‡å®šè¾“å…¥ï¼Œå³å¯è·å¾—è¿”å›çš„è¾“å‡ºç»“æœã€‚
ä½•æ—¶ä½¿ç”¨æ­¤å·¥ä½œæµï¼š

å½“æ‹†åˆ†åçš„å­ä»»åŠ¡å¯ä»¥ä¸ºäº†æé«˜é€Ÿåº¦è€Œå¹¶è¡Œå¤„ç†ï¼ˆåˆ†æ®µæ¨¡å¼ï¼‰ï¼Œæˆ–è€…å½“éœ€è¦å¤šä¸ªè§†è§’/å°è¯•ä»¥è·å¾—æ›´é«˜ç½®ä¿¡åº¦çš„ç»“æœï¼ˆæŠ•ç¥¨æ¨¡å¼ï¼‰æ—¶ï¼Œå¹¶è¡ŒåŒ–æ–¹æ¡ˆéå¸¸æœ‰æ•ˆã€‚
åº”ç”¨ç¤ºä¾‹ï¼š

1. åˆ†æ®µæ¨¡å¼ (Sectioning):

    å®æ–½æŠ¤æ  (Guardrails)ï¼š ä¸€ä¸ªæ¨¡å‹å®ä¾‹å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œè€Œå¦ä¸€ä¸ªå®ä¾‹åŒæ—¶ç­›é€‰å…¶ä¸­æ˜¯å¦åŒ…å«ä¸å½“å†…å®¹æˆ–è¿è§„è¯·æ±‚ã€‚

    è‡ªåŠ¨åŒ–è¯„ä¼° (Evals)ï¼š åœ¨è¯„ä¼° LLM æ€§èƒ½æ—¶ï¼Œé€šè¿‡å¤šä¸ª LLM è°ƒç”¨åˆ†åˆ«è¯„ä¼°æ¨¡å‹åœ¨ç‰¹å®šæç¤ºè¯ä¸‹ä¸åŒç»´åº¦çš„è¡¨ç°ã€‚

2. æŠ•ç¥¨æ¨¡å¼ (Voting):

    ä»£ç æ¼æ´å®¡æŸ¥ï¼š å¤šä¸ªä¸åŒçš„æ™ºèƒ½ä½“ï¼ˆAgentï¼‰å…±åŒå®¡æŸ¥ä¸€æ®µä»£ç ï¼Œåªè¦å‘ç°é—®é¢˜å°±è¿›è¡Œæ ‡è®°ã€‚

    å†…å®¹åˆè§„æ€§è¯„ä¼°ï¼š å¤šä¸ªæ™ºèƒ½ä½“ä»ä¸åŒç»´åº¦è¯„ä¼°å†…å®¹æ˜¯å¦è¿è§„ï¼Œå¹¶é€šè¿‡è®¾ç½®ä¸åŒçš„æŠ•ç¥¨é˜ˆå€¼ï¼Œæ¥å¹³è¡¡è¯¯æŠ¥ç‡ï¼ˆFalse Positivesï¼‰å’Œæ¼æŠ¥ç‡ï¼ˆNegativeï¼‰ã€‚
```

ç„¶åæ¥çœ‹ä¸€ä¸‹ç”¨åˆ°çš„ï¼š`generate`

ç„¶åå°±å‘ç°æˆ‘ä»¬åˆšåˆšçš„ç†è§£æœ‰æ­§ä¹‰ï¼Œå…¶å®æ˜¯æŠŠfan_outçš„ä¿¡æ¯äº¤ç»™fan_inæ±‡æ€»

å¥½çš„ï¼Œ`ParallelLLM`æˆ‘ä»¬äº†è§£äº†ï¼Œè¿™é‡Œçš„å†™æ³•å¹¶ä¸éš¾ç†è§£ï¼Œå¤§å®¶æ„Ÿå…´è¶£å¯ä»¥è‡ªå·±ç‚¹è¿›å»çœ‹

é‚£è¿™é‡Œç›¸å½“äºæ˜¯å…ˆè·‘`concept_analysis`å’Œ`algorithm_analysis`ï¼Œç„¶åæŠŠç»“æœç»™`code_planning`æ±‡æ€»å‡ºæ¥ã€‚

å¥½ï¼Œäº†è§£äº†ï¼Œé‚£ä¹ˆæˆ‘ä»¬åªç”¨çœ‹è¿™ä¸‰ä¸ªagentçš„æç¤ºè¯å’Œä¼ å…¥çš„messageå°±è§£å†³è¿™ä¸ªphaseäº†

#### concept_analysis

æç¤ºè¯ï¼š

``` markdown
You are doing a COMPREHENSIVE analysis of a research paper to understand its complete structure, contributions, and implementation requirements.

# OBJECTIVE
Map out the ENTIRE paper structure and identify ALL components that need implementation for successful reproduction.

# INTELLIGENT DOCUMENT READING STRATEGY

## IMPORTANT: Use Segmented Reading for Optimal Performance
Instead of reading the entire document at once (which may hit token limits), use the intelligent segmentation system:

1. **Use read_document_segments tool** with these parameters:
   - query_type: "concept_analysis"
   - keywords: ["introduction", "overview", "architecture", "system", "framework", "concept", "method"]
   - max_segments: 3
   - max_total_chars: 6000

2. **This will automatically find and retrieve** the most relevant sections for concept analysis without token overflow

3. **If you need additional sections**, make follow-up calls with different keywords like ["experiment", "evaluation", "results"] or ["conclusion", "discussion"]

# COMPREHENSIVE ANALYSIS PROTOCOL

## 1. INTELLIGENT PAPER STRUCTURAL ANALYSIS
Use the segmented reading approach to create a complete map:

paper_structure_map:
  title: "[Full paper title]"

  sections:
    1_introduction:
      main_claims: "[What the paper claims to achieve]"
      problem_definition: "[Exact problem being solved]"

    2_related_work:
      key_comparisons: "[Methods this work builds upon or competes with]"

    3_method:  # May have multiple subsections
      subsections:
        3.1: "[Title and main content]"
        3.2: "[Title and main content]"
      algorithms_presented: "[List all algorithms by name]"

    4_experiments:
      environments: "[All test environments/datasets]"
      baselines: "[All comparison methods]"
      metrics: "[All evaluation metrics used]"

    5_results:
      main_findings: "[Key results that prove the method works]"
      tables_figures: "[Important result tables/figures to reproduce]"

## 2. METHOD DECOMPOSITION
For the main method/approach:

method_decomposition:
  method_name: "[Full name and acronym]"

  core_components:  # Break down into implementable pieces
    component_1:
      name: "[e.g., State Importance Estimator]"
      purpose: "[Why this component exists]"
      paper_section: "[Where it's described]"

    component_2:
      name: "[e.g., Policy Refinement Module]"
      purpose: "[Its role in the system]"
      paper_section: "[Where it's described]"

  component_interactions:
    - "[How component 1 feeds into component 2]"
    - "[Data flow between components]"

  theoretical_foundation:
    key_insight: "[The main theoretical insight]"
    why_it_works: "[Intuitive explanation]"

## 3. IMPLEMENTATION REQUIREMENTS MAPPING
Map paper content to code requirements:

implementation_map:
  algorithms_to_implement:
    - algorithm: "[Name from paper]"
      section: "[Where defined]"
      complexity: "[Simple/Medium/Complex]"
      dependencies: "[What it needs to work]"

  models_to_build:
    - model: "[Neural network or other model]"
      architecture_location: "[Section describing it]"
      purpose: "[What this model does]"

  data_processing:
    - pipeline: "[Data preprocessing needed]"
      requirements: "[What the data should look like]"

  evaluation_suite:
    - metric: "[Metric name]"
      formula_location: "[Where it's defined]"
      purpose: "[What it measures]"

## 4. EXPERIMENT REPRODUCTION PLAN
Identify ALL experiments needed:

experiments_analysis:
  main_results:
    - experiment: "[Name/description]"
      proves: "[What claim this validates]"
      requires: "[Components needed to run this]"
      expected_outcome: "[Specific numbers/trends]"

  ablation_studies:
    - study: "[What is being ablated]"
      purpose: "[What this demonstrates]"

  baseline_comparisons:
    - baseline: "[Method name]"
      implementation_required: "[Yes/No/Partial]"
      source: "[Where to find implementation]"

## 5. CRITICAL SUCCESS FACTORS
What defines successful reproduction:

success_criteria:
  must_achieve:
    - "[Primary result that must be reproduced]"
    - "[Core behavior that must be demonstrated]"

  should_achieve:
    - "[Secondary results that validate the method]"

  validation_evidence:
    - "[Specific figure/table to reproduce]"
    - "[Qualitative behavior to demonstrate]"

# OUTPUT FORMAT
comprehensive_paper_analysis:
  executive_summary:
    paper_title: "[Full title]"
    core_contribution: "[One sentence summary]"
    implementation_complexity: "[Low/Medium/High]"
    estimated_components: "[Number of major components to build]"

  complete_structure_map:
    [FULL SECTION BREAKDOWN AS ABOVE]

  method_architecture:
    [DETAILED COMPONENT BREAKDOWN]

  implementation_requirements:
    [ALL ALGORITHMS, MODELS, DATA, METRICS]

  reproduction_roadmap:
    phase_1: "[What to implement first]"
    phase_2: "[What to build next]"
    phase_3: "[Final components and validation]"

  validation_checklist:
    - "[ ] [Specific result to achieve]"
    - "[ ] [Behavior to demonstrate]"
    - "[ ] [Metric to match]"

BE THOROUGH. Miss nothing. The output should be a complete blueprint for reproduction.
```

ç¿»è¯‘ä¸æ”¾äº†ï¼Œå¤§å®¶å“ªé‡Œçœ‹ä¸æ‡‚è‡ªå·±ç¿»è¯‘å§ï¼Œä¸‹é¢æˆ‘ä»¬æ¥è§£æè¿™æ®µæç¤ºè¯

é¦–å…ˆæ˜¯ç»å…¸èº«ä»½è®¾å®šï¼šYou are doing a COMPREHENSIVE analysis of a research paper to understand its complete structure, contributions, and implementation requirements.

è¯´æ˜ç™½äº†æ­£åœ¨å¹²å—ï¼Œè¦äº†è§£è®ºæ–‡çš„ç»“æ„ã€è´¡çŒ®å’Œå®æ–½è¦æ±‚

ç„¶åæ˜¯è¯´äº†æœ€åè¦è¾“å‡ºçš„ç»“æœï¼š

ç»˜åˆ¶å‡ºæ•´ä¸ªè®ºæ–‡ç»“æ„å¹¶ç¡®å®šæˆåŠŸå¤åˆ¶æ‰€éœ€å®æ–½çš„æ‰€æœ‰ç»„ä»¶ã€‚

æ¥ä¸‹æ¥æœ‰ä¸‰ä¸ªéƒ¨åˆ†ï¼š`INTELLIGENT DOCUMENT READING STRATEGYï¼ˆæŒ‡å¯¼agentæ€ä¹ˆè¯»æ–‡ç« ï¼‰`ã€`COMPREHENSIVE ANALYSIS PROTOCOLï¼ˆç”¨æ¥åˆ†æçš„å·¥ä½œæµï¼‰`å’Œ`OUTPUT FORMATï¼ˆè¾“å‡ºæ ¼å¼ï¼‰`

##### INTELLIGENT DOCUMENT READING STRATEGY

å¥½çš„ï¼Œè¿™ä¸ªå·¥ä½œæµæŒ‡å¯¼agentæ€ä¹ˆå»ç”¨å·¥å…·è¯»åˆ†å—ï¼Œå¹¶ä¸”æŒ‡å¯¼agentæ€ä¹ˆå¾ªç¯åˆ†æï¼Œè¿™ä¸ªå¾ˆæœ‰æ„æ€ï¼ŒæŠŠæ‰€æœ‰å†³ç­–æƒäº¤ç»™llmï¼Œå¦‚æœæ˜¯æˆ‘çš„è¯å¯èƒ½ä¼šç”¨å›ºå®šçš„workflowæ¥åšæ•´ä¸ªè¿‡ç¨‹

##### COMPREHENSIVE ANALYSIS PROTOCOL

ç¬¬ä¸€æ­¥ï¼Œç”¨åˆšåˆšæ•™çš„æ–¹æ³•å»åˆ†å—è¯»ï¼Œç„¶ååˆ›å»ºä¸€ä¸ªyamlè®°å½•ä¿¡æ¯ï¼Œè¿™é‡Œè¿˜æ˜¯ç»“æ„åŒ–æŒ‡å¯¼ï¼ˆè™½ç„¶åªç”¨äºå†…éƒ¨ä¼ å¯¼ï¼‰å…¶å®ä¹Ÿæ˜¯ä¸€ç§é˜²è ¢ï¼Œä»¥å…aiæ¼ä¿¡æ¯ç­‰ç­‰ã€‚çœ‹ä¸€ä¸‹è¿™ä¸ªyml

``` yaml
```yaml
paper_structure_map:
  title: "[Full paper title]"

  sections:
    1_introduction:
      main_claims: "[What the paper claims to achieve]"
      problem_definition: "[Exact problem being solved]"

    2_related_work:
      key_comparisons: "[Methods this work builds upon or competes with]"

    3_method:  # May have multiple subsections
      subsections:
        3.1: "[Title and main content]"
        3.2: "[Title and main content]"
      algorithms_presented: "[List all algorithms by name]"

    4_experiments:
      environments: "[All test environments/datasets]"
      baselines: "[All comparison methods]"
      metrics: "[All evaluation metrics used]"

    5_results:
      main_findings: "[Key results that prove the method works]"
      tables_figures: "[Important result tables/figures to reproduce]"
```

ç¬¬äºŒæ­¥å¯¹æ–¹æ³•åˆ†è§£ï¼Œè¿™ä¸€å—æˆ‘ä¸å¤ªæ‡‚ï¼ˆå› ä¸ºæˆ‘ä¸æå­¦æœ¯ä¹Ÿä¸çœ‹è®ºæ–‡ï¼‰

ç¬¬ä¸‰æ­¥å°±æ˜¯ç¡®å®šå…·ä½“è¦å®ç°å“ªäº›ä¸œè¥¿

ç¬¬å››æ­¥æ˜¯é‡ç°è®¡åˆ’ï¼Œç¬¬äº”éƒ¨æ˜¯éªŒè¯æˆåŠŸçš„éœ€è¦ï¼Œè¾“å‡ºå°±æ˜¯ä¸Šé¢çš„æ±‡æ€»

å¯ä»¥çœ‹åˆ°æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„æç¤ºè¯ç»“æ„ï¼Œæœ‰å®šä¹‰ï¼Œæœ‰å·¥å…·ä½¿ç”¨è¯´æ˜ï¼Œæœ‰æ‰§è¡Œæµç¨‹

#### algorithm_analysis

æç¤ºè¯æˆ‘ä¸ç²˜è´´äº†ï¼Œå¤§å®¶åˆšåˆšä¹Ÿçœ‹åˆ°æ€ä¹ˆå»æ‰¾æç¤ºè¯äº†ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹è¿™ä¸ªæç¤ºè¯

è¿™ä¸ªæç¤ºè¯è·Ÿåˆšåˆšåˆ†æçš„é‚£ä¸ªç»“æ„éå¸¸ç›¸ä¼¼ï¼Œå¤§å®¶å¯ä»¥è‡ªå·±åˆ†æåˆ†æï¼Œæœ‰ä¸æ‡‚çš„æ¬¢è¿æ¥æ‰¾æˆ‘è®¨è®ºï¼Œæœ€ç»ˆè¿™ä¸ªagentçš„ç›®çš„æ˜¯ä¸ºäº†åˆ†æç”¨åˆ°çš„ç®—æ³•å’Œå…¬å¼

#### code_planning

å¥½ï¼Œæ¥çœ‹ä¸€ä¸‹è®¡åˆ’è§„åˆ’agentï¼Œè¿™ä¸ªå¾ˆé‡è¦ï¼Œmdè¿™ä¸ªæç¤ºè¯ä¹Ÿç‰¹åˆ«é•¿ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ï¼ˆè¿™ä¸‹çŸ¥é“ä¸ºå•¥è¿™äº›å·¥å…·è¿™ä¹ˆçƒ§tokenäº†å§ï¼‰

é¦–å…ˆè¯´æ˜èƒŒæ™¯ï¼Œé€šè¿‡åˆ†ææ¥åˆ¶å®šå¯ä»¥å¤åˆ»çš„è®¡åˆ’ï¼Œæ¥ä¸‹æ¥åˆ†ä¸ºï¼š`INPUTï¼ˆè¯´æ˜è¾“å…¥æ ¼å¼ï¼‰`ã€`INTELLIGENT DOCUMENT ACCESSï¼ˆæ™ºèƒ½æ–‡æ¡£è®¿é—®ï¼‰`ã€`OBJECTIVEï¼ˆæœ€ç»ˆè¦å®ç°çš„ç›®æ ‡ï¼‰`ã€`CONTENT LENGTH CONTROLï¼ˆç”Ÿæˆå†…å®¹å’Œé•¿åº¦è¯´æ˜ï¼‰`ã€`DETAILED SYNTHESIS PROCESSï¼ˆå…·ä½“çš„æ‰§è¡Œæ–¹å¼ï¼Œä¹Ÿå°±æ˜¯workflowï¼‰`ã€`COMPREHENSIVE OUTPUT FORMATï¼ˆæœ€ç»ˆçš„è¾“å‡ºæ ¼å¼ï¼‰`

##### INPUT

è¯´æ˜äº†ä¸€ä¸‹ä¼ è¿›æ¥çš„åˆ†ææœ‰ä¸¤ä¸ªï¼Œä¹Ÿå°±æ˜¯ä»‹ç»äº†ä¸€ä¸‹åˆšåˆšè¿›è¡Œçš„ä¸¤ä¸ªå·¥ä½œ

##### INTELLIGENT DOCUMENT ACCESS

è¯´æ˜äº†æ€ä¹ˆç”¨mcpå»è¯»åˆ†ç‰‡

##### OBJECTIVE

æœ€ç»ˆè¦å®ç°çš„ç›®æ ‡ï¼Œåˆ›å»ºä¸€ä¸ªå¦‚æ­¤è¯¦ç»†çš„å®æ–½è®¡åˆ’ï¼Œä»¥ä¾¿å¼€å‘äººå‘˜å¯ä»¥åœ¨ä¸é˜…è¯»æ•´ç¯‡è®ºæ–‡çš„æƒ…å†µä¸‹å¤åˆ¶å®ƒã€‚

##### CONTENT LENGTH CONTROL

è¯´æ˜äº†è¦åˆ›å»ºå“ªå‡ ä¸ªæ–‡ä»¶éƒ¨åˆ†ï¼Œå’Œæ¯ä¸ªéƒ¨åˆ†çš„å¤§æ¦‚å æ¯”

1. åˆ›å»ºæ–‡ä»¶ç»“æ„ï¼Œä¸€å®šè¦å®Œæ•´
2. å®ç°ç»„ä»¶ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
3. æœ€ç»ˆçš„éªŒè¯
4. æ‰€æœ‰éœ€è¦çš„ä¾èµ–ï¼ˆæŒ‡å¼€å‘ç¯å¢ƒçš„ï¼‰
5. ä¸€æ­¥ä¸€æ­¥çš„å®ç°ç»†èŠ‚

ç„¶åè¯´æ˜äº†ä¸€ä¸‹å®ç°æ­¥éª¤ï¼Œå…ˆç®—æ³•æ–‡ä»¶ï¼Œç„¶åæ˜¯å·¥å…·ç±»ï¼Œç„¶åæ˜¯æµ‹è¯•è„šæœ¬ï¼Œç„¶åæ˜¯é…ç½®å’Œæ•°æ®å¤„ç†ï¼Œç„¶åæ‰æ˜¯è¯´æ˜æ–‡æ¡£ï¼Œç„¶åè¯´æ˜äº†è¦åŒ…å«READMEå’Œrequirements.txtï¼ˆå†™è®ºæ–‡çš„å–œæ¬¢ç”¨pythonå®ç°ï¼Œæ‰€ä»¥è¯¥å·¥å…·å°±æ˜¯ä¸“æ³¨äºç”Ÿæˆpythonä»£ç ï¼‰

##### DETAILED SYNTHESIS PROCESS

1. è¯´æ˜äº†è¦åˆå¹¶å¾—åˆ°çš„æ‰€æœ‰ä¿¡æ¯ï¼Œç„¶åè¯´äº†ä¸€ä¸‹å“ªäº›
2. ç„¶åè¦æŠŠå¾—åˆ°çš„ä¿¡æ¯å˜æˆå®ç°
3. æŠ€æœ¯æå–çš„æ–¹æ³•

##### COMPREHENSIVE OUTPUT FORMAT

è¾“å‡ºæ ¼å¼ï¼Œå¤§å®¶è‡ªå·±çœ‹å§ï¼Œå°±æ˜¯æŠŠä¸Šé¢å¾—åˆ°çš„ç»“æœæ±‡æ€»è¾“å‡º

---

å¥½çš„ï¼Œæ¼«é•¿çš„phase4ç»“æŸäº†ï¼Œæœ€ç»ˆå¾—åˆ°çš„ç»“æœä¿å­˜åˆ°äº†`initial_plan.txt`
