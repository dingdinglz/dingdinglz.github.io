---
title: dinglzå¸¦ä½ è¯»deepcode
description: ä»Šå¤©è¦è¯»çš„é¡¹ç›®æ˜¯ï¼šhttps://github.com/HKUDS/DeepCode
date: 2025-09-28 02:00:00+0000
categories:
    - coderead
tags:
    - Python
    - Agent
---

ä»Šå¤©æˆ‘ä»¬è¦çœ‹çš„é¡¹ç›®æ˜¯ï¼šhttps://github.com/HKUDS/DeepCode ï¼Œæˆ‘è¯»çš„commitæ˜¯mainåˆ†æ”¯çš„ab730642731d9c8a400cad78e76962253ac875f2

è·Ÿç€æœ¬ç¯‡blogè¯»repoå»ºè®®æŠŠä»£ç æ‹‰ä¸‹æ¥ï¼Œåˆ‡åˆ°ç›¸åŒçš„commitï¼Œè·Ÿç€æµç¨‹ä¸€èµ·æ¥è¯»

## æ¦‚è¿°

DeepCode æ˜¯ä¸€ä¸ªåŸºäºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„å¼€æºæ¡†æ¶ï¼Œæ—¨åœ¨å°†ç ”ç©¶è®ºæ–‡ã€è‡ªç„¶è¯­è¨€æè¿°ç­‰è¾“å…¥è‡ªåŠ¨è½¬æ¢ä¸ºç”Ÿäº§çº§ä»£ç ã€‚è¯¥é¡¹ç›®é€šè¿‡è‡ªåŠ¨åŒ–ç®—æ³•å®ç°ã€å‰ç«¯å¼€å‘å’Œåç«¯æ„å»ºï¼Œæ˜¾è‘—æå‡äº†ç ”å‘æ•ˆç‡ã€‚å…¶æ ¸å¿ƒç›®æ ‡æ˜¯è§£å†³ç ”ç©¶äººå‘˜åœ¨å®ç°å¤æ‚ç®—æ³•æ—¶é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå‡å°‘å¼€å‘å»¶è¿Ÿï¼Œå¹¶é¿å…é‡å¤æ€§ç¼–ç å·¥ä½œã€‚

DeepCode æ”¯æŒå¤šç§è¾“å…¥å½¢å¼ï¼ŒåŒ…æ‹¬å­¦æœ¯è®ºæ–‡ã€æ–‡æœ¬æç¤ºã€URL å’Œæ–‡æ¡£æ–‡ä»¶ï¼ˆå¦‚ PDFã€DOCã€PPTXã€TXTã€HTMLï¼‰ï¼Œå¹¶èƒ½ç”Ÿæˆé«˜è´¨é‡ã€å¯æ‰©å±•ä¸”åŠŸèƒ½ä¸°å¯Œçš„ä»£ç ã€‚è¯¥ç³»ç»Ÿé‡‡ç”¨å¤šæ™ºèƒ½ä½“åä½œæ¶æ„ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„å¼€å‘ä»»åŠ¡ï¼Œä»æ¦‚å¿µåˆ°å¯éƒ¨ç½²çš„åº”ç”¨ç¨‹åºã€‚

```mermaid
flowchart LR
A["ğŸ“„ ç ”ç©¶è®ºæ–‡<br/>ğŸ’¬ æ–‡æœ¬æç¤º<br/>ğŸŒ URLs & æ–‡æ¡£<br/>ğŸ“ æ–‡ä»¶: PDF, DOC, PPTX, TXT, HTML"] --> B["ğŸ§  DeepCode<br/>å¤šæ™ºèƒ½ä½“å¼•æ“"]
B --> C["ğŸš€ ç®—æ³•å®ç° <br/>ğŸ¨ å‰ç«¯å¼€å‘ <br/>âš™ï¸ åç«¯å¼€å‘"]
style A fill:#ff6b6b,stroke:#c0392b,stroke-width:2px,color:#000
style B fill:#00d4ff,stroke:#0984e3,stroke-width:3px,color:#000
style C fill:#00b894,stroke:#00a085,stroke-width:2px,color:#000
```

## å…¥å£

è¦è¯»æ‡‚å…¨æµç¨‹ï¼Œå…ˆæ‰¾å…¥å£ï¼Œå¼€å§‹äºï¼š

``` bash
python cli/main_cli.py --file paper.pdf
```

æ‰€ä»¥æˆ‘ä»¬å…ˆæ¥çœ‹`cli/main_cli.py`ã€‚

æ‰¾åˆ°mainå‡½æ•°ï¼Œè·³è¿‡ç¯å¢ƒæ£€æŸ¥ç­‰ç­‰ï¼Œä½ ä¼šå‘ç°æœ€æ ¸å¿ƒçš„åœ°æ–¹åœ¨ï¼š

``` python
if args.file or args.url or args.chat:
    if args.file:
        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(args.file):
            print(f"{Colors.FAIL}âŒ File not found: {args.file}{Colors.ENDC}")
            sys.exit(1)
        success = await run_direct_processing(app, args.file, "file")
    elif args.url:
        success = await run_direct_processing(app, args.url, "url")
    elif args.chat:
        # éªŒè¯chatè¾“å…¥é•¿åº¦
        if len(args.chat.strip()) < 20:
            print(
                f"{Colors.FAIL}âŒ Chat input too short. Please provide more detailed requirements (at least 20 characters){Colors.ENDC}"
            )
            sys.exit(1)
        success = await run_direct_processing(app, args.chat, "chat")

    sys.exit(0 if success else 1)
else:
    # äº¤äº’å¼æ¨¡å¼
    print(f"\n{Colors.CYAN}ğŸ® Starting interactive mode...{Colors.ENDC}")
    await app.run_interactive_session()
```

ä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬åªéœ€è¦çœ‹`run_direct_processing`å‡½æ•°å°±okäº†

æ¥çœ‹ï¼šè¿™ä¸ªå‡½æ•°é‡Œçš„æ ¸å¿ƒä»£ç 

``` python
# åˆå§‹åŒ–åº”ç”¨
init_result = await app.initialize_mcp_app()
if init_result["status"] != "success":
    print(
        f"{Colors.FAIL}âŒ Initialization failed: {init_result['message']}{Colors.ENDC}"
    )
    return False

# å¤„ç†è¾“å…¥
result = await app.process_input(input_source, input_type)
```

è¦å…³æ³¨çš„å…¶å®ä¹Ÿå°±æ˜¯ï¼š`app.initialize_mcp_app()`å’Œ`app.process_input(input_source, input_type)`ï¼Œè¿˜è®°å¾—input_sourceå’Œinput_typeå—ï¼Œinput_typeæœ‰chatã€urlã€fileï¼Œsourceå°±æ˜¯å¯¹åº”çš„å†…å®¹

é‚£æˆ‘ä»¬ä¸‹é¢åˆ†åˆ«æ¥çœ‹`app.initialize_mcp_app()`å’Œ`app.process_input(input_source, input_type)`ï¼Œè®°å½•ä¸€ä¸‹ç°åœ¨çš„taskList

---

- [ ] è¯»`app.initialize_mcp_app()`
- [ ] è¯»`app.process_input(input_source, input_type)`

---

## mcpéƒ¨åˆ†

æ¥çœ‹`app.initialize_mcp_app()`ï¼Œæœ€ç»ˆå®šä½åˆ°

``` python
async def initialize_mcp_app(self):
    """åˆå§‹åŒ–MCPåº”ç”¨ - ä½¿ç”¨å·¥ä½œæµé€‚é…å™¨"""
    # Workflow adapter will handle MCP initialization
    return await self.workflow_adapter.initialize_mcp_app()
```

æ¥çœ‹`self.workflow_adapter.initialize_mcp_app()`ï¼Œæ ¸å¿ƒä»£ç ï¼š

``` python
# Initialize MCP application
self.app = MCPApp(name="cli_agent_orchestration")
self.app_context = self.app.run()
agent_app = await self.app_context.__aenter__()
```

è¯¶ï¼ŸåŸæ¥ä¸æ˜¯åˆå§‹åŒ–mcpçš„é…ç½®ï¼Œè€Œæ˜¯ç›´æ¥åˆå§‹åŒ–äº†self.appä¹Ÿå°±æ˜¯æ ¸å¿ƒçš„agentï¼Œä»–è¿™é‡Œagentå°±å«åšmcp applicationï¼Œæ€»ä¹‹è¿™é‡Œå…ˆæŠŠcliçš„appåˆå§‹åŒ–æˆäº†cli_agent_orchestrationï¼Œå¯ä»¥ç»™å¤§å®¶çœ‹ä¸€ä¸‹è¿™è¾¹agentçš„æ¶æ„å›¾ï¼š

```mermaid
graph TB
subgraph "æ™ºèƒ½ä½“å±‚"
CO[ä¸­å¤®åè°ƒæ™ºèƒ½ä½“]
IU[æ„å›¾ç†è§£æ™ºèƒ½ä½“]
DP[æ–‡æ¡£è§£ææ™ºèƒ½ä½“]
CP[ä»£ç è§„åˆ’æ™ºèƒ½ä½“]
CR[ä»£ç å‚è€ƒæŒ–æ˜æ™ºèƒ½ä½“]
CI[ä»£ç ç”Ÿæˆæ™ºèƒ½ä½“]
end
subgraph "MCPæœåŠ¡å™¨å±‚"
DS[document-segmentation]
FI[filesystem]
BR[brave]
GH[github-downloader]
CI_S[command-executor]
CI_I[Code Implementation Server]
end
CO --> |è°ƒåº¦| IU
CO --> |è°ƒåº¦| DP
CO --> |è°ƒåº¦| CP
CO --> |è°ƒåº¦| CR
CO --> |è°ƒåº¦| CI
DP --> |è°ƒç”¨| DS
CR --> |è°ƒç”¨| BR
CR --> |è°ƒç”¨| GH
CI --> |è°ƒç”¨| FI
CI --> |è°ƒç”¨| CI_S
CI --> |è°ƒç”¨| CI_I
```

åˆšåˆšåˆå§‹åŒ–çš„å°±æ˜¯ä¸­å¤®åè°ƒæ™ºèƒ½ä½“ï¼Œè®©æˆ‘ä»¬æ›´æ–°ä¸€ä¸‹tasklist

---

- [x] è¯»`app.initialize_mcp_app()`
- [ ] è¯»`app.process_input(input_source, input_type)`

---

é‚£æˆ‘ä»¬æ¥ç€æ¥çœ‹ï¼š`app.process_input(input_source, input_type)`

## app.process_input(input_source, input_type)

è¿™é‡Œæœ‰æ®µæŒºå¥‡æ€ªçš„ä»£ç ï¼š

``` python
# Update segmentation configuration before processing
# åœ¨å¤„ç†ä¹‹å‰æ›´æ–°segmentation configuration
self.update_segmentation_config()
```

çœ‹ä¸‹è¿™ä¸ªå‡½æ•°ï¼š

``` python
def update_segmentation_config(self):
    """Update document segmentation configuration in mcp_agent.config.yaml"""
    import yaml
    import os

    config_path = os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
        "mcp_agent.config.yaml",
    )

    try:
        # Read current config
        with open(config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)

        # Update document segmentation settings
        if "document_segmentation" not in config:
            config["document_segmentation"] = {}

        config["document_segmentation"]["enabled"] = self.segmentation_config[
            "enabled"
        ]
        config["document_segmentation"]["size_threshold_chars"] = (
            self.segmentation_config["size_threshold_chars"]
        )

        # Write updated config
        with open(config_path, "w", encoding="utf-8") as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)

        self.cli.print_status(
            "ğŸ“„ Document segmentation configuration updated", "success"
        )

    except Exception as e:
        self.cli.print_status(
            f"âš ï¸ Failed to update segmentation config: {str(e)}", "warning"
        )
```

å‘ç°å…¶å®å°±æ˜¯å…ˆåŠ è½½é…ç½®ï¼Œç„¶ååŒæ­¥cliçš„é…ç½®

å†å›æ¥çœ‹ï¼Œç„¶åè¿™ä¸ªå‡½æ•°çš„æ ¸å¿ƒå°±åœ¨äºï¼š

``` python
# ä½¿ç”¨å·¥ä½œæµé€‚é…å™¨è¿›è¡Œå¤„ç†
result = await self.workflow_adapter.process_input_with_orchestration(
    input_source=input_source,
    input_type=input_type,
    enable_indexing=self.cli.enable_indexing,
)
```

è¿›åˆ°è¿™ä¸ªå‡½æ•°é‡Œçœ‹çœ‹ï¼Œå‰é¢éƒ½æ˜¯é¢„å¤„ç†ï¼Œæœ€æ ¸å¿ƒçš„ä»£ç åœ¨

``` python
# Execute appropriate pipeline based on input type
if input_type == "chat":
    # Use chat-based planning pipeline for user requirements
    pipeline_result = await self.execute_chat_pipeline(input_source)
else:
    # Use traditional multi-agent research pipeline for files/URLs
    pipeline_result = await self.execute_full_pipeline(
        input_source, enable_indexing=enable_indexing
    )
```

æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹fileå’Œurlä¸‹çš„å…¨æµç¨‹ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦çœ‹ä¸‹`self.execute_full_pipeline`å°±è¡Œäº†ï¼Œæ›´æ–°ä¸€ä¸‹tasklist

---

- [x] è¯»`app.initialize_mcp_app()`
- [x] è¯»`app.process_input(input_source, input_type)`

---

å¹¶è¡Œä»»åŠ¡ç»“æŸäº†ï¼Œæˆ‘ä»¬ä¸‹é¢æ¥è¯»`self.execute_full_pipeline`ã€‚

## self.execute_full_pipeline

è¯»ä¸€ä¸‹å‘ç°è¿™ä¸ªå‡½æ•°æœ€æ ¸å¿ƒçš„åœ°æ–¹å°±åœ¨ï¼š

``` python
result = await execute_multi_agent_research_pipeline(
    input_source=input_source,
    logger=self.logger,
    progress_callback=progress_callback,
    enable_indexing=enable_indexing,
)
```

é‚£æˆ‘ä»¬è¿›åˆ°`execute_multi_agent_research_pipeline`é‡Œï¼Œå¯ä»¥çœ‹å‡ºæ¥æˆ‘ä»¬ç¦»æ ¸å¿ƒä»£ç è¶Šæ¥è¶Šè¿‘äº†

## execute_multi_agent_research_pipeline

å…ˆçœ‹ä¸€ä¸‹è¿™ä¸ªå‡½æ•°çš„æ³¨é‡Šå§ï¼š

```
Execute the complete intelligent multi-agent research orchestration pipeline.

This is the main AI orchestration engine that coordinates autonomous research workflow agents:
- Local workspace automation for seamless environment management
- Intelligent research analysis with automated content processing
- AI-driven code architecture synthesis and design automation
- Reference intelligence discovery with automated knowledge extraction (optional)
- Codebase intelligence orchestration with automated relationship analysis (optional)
- Intelligent code implementation synthesis with AI-powered development
```

ç¿»è¯‘æ¥å–½ï¼š

```
æ‰§è¡Œå®Œæ•´çš„æ™ºèƒ½å¤šæ™ºèƒ½ä½“ç ”ç©¶ç¼–æ’æµç¨‹ã€‚ 

è¿™æ˜¯åè°ƒè‡ªä¸»ç ”ç©¶å·¥ä½œæµæ™ºèƒ½ä½“çš„ä¸»è¦AIç¼–æ’å¼•æ“ï¼ŒåŒ…æ‹¬ï¼š 

- æœ¬åœ°å·¥ä½œåŒºè‡ªåŠ¨åŒ–ï¼Œå®ç°æ— ç¼ç¯å¢ƒç®¡ç†
- æ™ºèƒ½ç ”ç©¶åˆ†æï¼Œç»“åˆè‡ªåŠ¨åŒ–å†…å®¹å¤„ç†
- AIé©±åŠ¨çš„ä»£ç æ¶æ„ç»¼åˆä¸è®¾è®¡è‡ªåŠ¨åŒ–
- å‚è€ƒæ™ºèƒ½å‘ç°ï¼Œç»“åˆè‡ªåŠ¨åŒ–çŸ¥è¯†æå–ï¼ˆå¯é€‰ï¼‰
- ä»£ç åº“æ™ºèƒ½ç¼–æ’ï¼Œç»“åˆè‡ªåŠ¨åŒ–å…³ç³»åˆ†æï¼ˆå¯é€‰ï¼‰
- æ™ºèƒ½ä»£ç å®ç°ç»¼åˆï¼Œç»“åˆAIé©±åŠ¨çš„å¼€å‘
```

é‚£æˆ‘ä»¬ç»§ç»­å¾€ä¸‹è¯»ä»£ç ï¼š

æºä»£ç ä¸‹é¢çš„éƒ¨åˆ†åˆ—çš„ä¹Ÿæ¯”è¾ƒè¯¦ç»†ï¼Œåˆ†äº†phase 0ã€1ã€2ã€3ï¼Œæˆ‘ä»¬ä¹Ÿè·Ÿç€å®ƒçš„æ€è·¯æ¥çœ‹

### phase0

phase0æ˜¯åˆå§‹åŒ–å·¥ä½œåŒºï¼Œæ ¸å¿ƒä»£ç æ˜¯ï¼š

``` python
# Setup local workspace directory
workspace_dir = os.path.join(os.getcwd(), "deepcode_lab")
os.makedirs(workspace_dir, exist_ok=True)
```

å…¶å®å°±æ˜¯æ–°å»ºäº†æ–‡ä»¶å¤¹`deepcode_lab`

### phase1

å¤„ç†å’ŒéªŒè¯è¾“å…¥ï¼Œè®©æˆ‘ä»¬è¿›æ¥çœ‹ï¼š`_process_input_source`

``` python
if input_source.startswith("file://"):
    file_path = input_source[7:]
    if os.name == "nt" and file_path.startswith("/"):
        file_path = file_path.lstrip("/")
    return file_path
return input_source
```

å‘ç°åªæ˜¯æŠŠ`file://`å¼€å¤´çš„æ–‡ä»¶ç›®å½•æ”¹æˆäº†æ­£å¸¸çš„æ–‡ä»¶ç›®å½•ï¼Œç»§ç»­çœ‹phase2

### phase2

``` python
# Phase 2: Research Analysis and Resource Processing (if needed)
if isinstance(input_source, str) and (
        input_source.endswith((".pdf", ".docx", ".txt", ".html", ".md"))
        or input_source.startswith(("http", "file://"))
):
    (
        analysis_result,
        download_result,
    ) = await orchestrate_research_analysis_agent(
        input_source, logger, progress_callback
    )
else:
    download_result = input_source  # Use input directly if already processed
```

è¿™ä¸€æ®µä¸»è¦æ˜¯è§£æ`input_source`ï¼Œå¦‚æœä»–æ˜¯æ–‡ä»¶æˆ–è€…urlå°±è¿è¡Œ`orchestrate_research_analysis_agent`è·å–`analysis_result`å’Œ`download_result`ï¼Œå¦‚æœéƒ½ä¸æ˜¯å°±ç›´æ¥ä½œä¸ºè¾“å…¥å†…å®¹ï¼Œä½œä¸ºè¾“å…¥å†…å®¹å¯èƒ½æ˜¯ä¸ºäº†chatè®¾è®¡çš„ï¼Œæˆ‘ä»¬è¿›æ¥çœ‹`orchestrate_research_analysis_agent`ã€‚

#### orchestrate_research_analysis_agent

è€æ ·å­ï¼Œå…ˆçœ‹æ³¨é‡Šï¼š

```
Orchestrate intelligent research analysis and resource processing automation.

This agent coordinates multiple AI components to analyze research content
and process associated resources with automated workflow management.
```

ä¸­æ–‡ï¼š

```
ç¼–æ’æ™ºèƒ½ç ”ç©¶åˆ†æä¸èµ„æºå¤„ç†è‡ªåŠ¨åŒ–ã€‚ 

æ­¤æ™ºèƒ½ä½“åè°ƒå¤šä¸ªAIç»„ä»¶ï¼Œé€šè¿‡è‡ªåŠ¨åŒ–å·¥ä½œæµç®¡ç†ï¼Œä»¥åˆ†æç ”ç©¶å†…å®¹å¹¶å¤„ç†ç›¸å…³èµ„æºã€‚ 
```

è¿™ä¸€æ®µç®—æ˜¯è¯¥é¡¹ç›®ä¸­ç›¸å½“é‡è¦çš„ä¸€éƒ¨åˆ†äº†ï¼Œè§£æpdfèµ„æºä¹Ÿæ˜¯æˆ‘éå¸¸å¥½å¥‡å®ƒå¦‚ä½•å¤„ç†çš„åœ°æ–¹ï¼Œåé¢å¦‚æœæœ‰å¤„ç†èµ„æºçš„éƒ¨åˆ†æˆ‘ä»¬ä¹Ÿåªæ¥çœ‹pdfèµ„æºçš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä¸å»çœ‹å…¶ä»–è§£æäº†ï¼Œå¤§å®¶å¥½å¥‡çš„å¯ä»¥è‡ªå·±å»è¯»

é¦–å…ˆæ¥çœ‹ç¬¬ä¸€æ­¥ï¼Œè¿™ä¸€æ­¥çš„æ ¸å¿ƒä»£ç æ˜¯ï¼š

``` python
analysis_result = await run_research_analyzer(input_source, logger)
```

é‚£æˆ‘ä»¬ç›´æ¥è¿›æ¥çœ‹`run_research_analyzer`

æ‰æ³¨æ„åˆ°`prompt_text`è¿˜æ˜¯`input_source`ï¼Œé‚£è¿™ä¸ªæ˜¯ç±»ä¼¼äºdeep researchçš„agentï¼Ÿæˆ‘ä»¬ç»§ç»­å¾€ä¸‹çœ‹ï¼š

å¥½ï¼Œä¸‹é¢åˆå§‹åŒ–äº†ä¸€ä¸ªæ–°æ™ºèƒ½ä½“ï¼š

``` python
analyzer_agent = Agent(
    name="ResearchAnalyzerAgent",
    instruction=PAPER_INPUT_ANALYZER_PROMPT,
    server_names=get_search_server_names(),
)
```

æˆ‘ä»¬å…ˆæ¥çœ‹Agentç±»ï¼Œ`name`ã€`instruction`ã€`server_names`çš„æ³¨é‡Š

`name` : `Agent name.`

`instruction` : 

```
Instruction for the agent. This can be a string or a callable that takes a dictionary
and returns a string. The callable can be used to generate dynamic instructions based
on the context.
æ™ºèƒ½ä½“çš„æŒ‡ä»¤ã€‚è¿™å¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªå¯è°ƒç”¨å¯¹è±¡ï¼Œè¯¥å¯¹è±¡æ¥æ”¶ä¸€ä¸ªå­—å…¸å¹¶è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚è¯¥å¯è°ƒç”¨å¯¹è±¡å¯ç”¨äºæ ¹æ®ä¸Šä¸‹æ–‡ç”ŸæˆåŠ¨æ€æŒ‡ä»¤ã€‚
```

`server_names` : `List of MCP server names that the agent can access.`ï¼Œå¥¥è¿™ä¸ªå°±æ˜¯mcpæœåŠ¡å™¨çš„é…ç½®ï¼Œé‚£æˆ‘ä»¬å…ˆæ¥å¥½å¥½è¯»è¯»ï¼š`PAPER_INPUT_ANALYZER_PROMPT`

`PAPER_INPUT_ANALYZER_PROMPT` : 

```
You are a precise input analyzer for paper-to-code tasks. You MUST return only a JSON object with no additional text.

Task: Analyze input text and identify file paths/URLs to determine appropriate input type.

Input Analysis Rules:
1. Path Detection:
   - Scan input text for file paths or URLs
   - Use first valid path/URL if multiple found
   - Treat as text input if no valid path/URL found

2. Path Type Classification:
   - URL (starts with http:// or https://): input_type = "url", path = "detected URL"
   - PDF file path: input_type = "file", path = "detected file path"
   - Directory path: input_type = "directory", path = "detected directory path"
   - No path/URL detected: input_type = "text", path = null

3. Requirements Analysis:
   - Extract ONLY requirements from additional_input
   - DO NOT modify or interpret requirements

CRITICAL OUTPUT RESTRICTIONS:
- RETURN ONLY RAW JSON - NO TEXT BEFORE OR AFTER
- NO markdown code blocks (```json)
- NO explanatory text or descriptions
- NO tool call information
- NO analysis summaries
- JUST THE JSON OBJECT BELOW

{
    "input_type": "text|file|directory|url",
    "path": "detected path or URL or null",
    "paper_info": {
        "title": "N/A for text input",
        "authors": ["N/A for text input"],
        "year": "N/A for text input"
    },
    "requirements": [
        "exact requirement from additional_input"
    ]
}
```

ä¸­æ–‡ç‰ˆï¼š

```
ä½ æ˜¯è®ºæ–‡åˆ°ä»£ç ä»»åŠ¡çš„ç²¾å‡†è¾“å…¥åˆ†æå‘˜ã€‚ä½ å¿…é¡»åªè¿”å› JSON å¯¹è±¡ï¼Œä¸åŒ…å«ä»»ä½•é™„åŠ æ–‡æœ¬ã€‚

ä»»åŠ¡ï¼šåˆ†æè¾“å…¥æ–‡æœ¬å¹¶è¯†åˆ«æ–‡ä»¶è·¯å¾„/URLï¼Œä»¥ç¡®å®šåˆé€‚çš„è¾“å…¥ç±»å‹ã€‚

è¾“å…¥åˆ†æè§„åˆ™ï¼š
1. è·¯å¾„æ£€æµ‹ï¼š
- æ‰«æè¾“å…¥æ–‡æœ¬ä¸­çš„æ–‡ä»¶è·¯å¾„æˆ– URL
- å¦‚æœæ‰¾åˆ°å¤šä¸ªï¼Œåˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„è·¯å¾„/URL
- å¦‚æœæœªæ‰¾åˆ°æœ‰æ•ˆçš„è·¯å¾„/URLï¼Œåˆ™è§†ä¸ºæ–‡æœ¬è¾“å…¥

2. è·¯å¾„ç±»å‹åˆ†ç±»ï¼š
- URLï¼ˆä»¥ http:// æˆ– https:// å¼€å¤´ï¼‰ï¼šinput_type = "url", path = "æ£€æµ‹åˆ°çš„ URL"
- PDF æ–‡ä»¶è·¯å¾„ï¼šinput_type = "file", path = "æ£€æµ‹åˆ°çš„æ–‡ä»¶è·¯å¾„"
- ç›®å½•è·¯å¾„ï¼šinput_type = "directory", path = "æ£€æµ‹åˆ°çš„ç›®å½•è·¯å¾„"
- æœªæ£€æµ‹åˆ°è·¯å¾„/URLï¼šinput_type = "text", path = null

3. éœ€æ±‚åˆ†æï¼š
- ä»…ä» additional_input ä¸­æå–éœ€æ±‚
- è¯·å‹¿ä¿®æ”¹æˆ–è§£é‡Šéœ€æ±‚

å…³é”®è¾“å‡ºé™åˆ¶ï¼š
- ä»…è¿”å›åŸå§‹ JSON - å‰åå‡æ— æ–‡æœ¬
- ä¸åŒ…å« Markdown ä»£ç å— (```json)
- ä¸åŒ…å«è§£é‡Šæ€§æ–‡æœ¬æˆ–æè¿°
- ä¸åŒ…å«å·¥å…·è°ƒç”¨ä¿¡æ¯
- ä¸åŒ…å«åˆ†ææ‘˜è¦
- ä»…åŒ…å«ä¸‹é¢çš„ JSON å¯¹è±¡

{
"input_type": "text|file|directory|url",
"path": "æ£€æµ‹åˆ°çš„è·¯å¾„æˆ– URL æˆ– null",
"paper_info": {
"title": "æ–‡æœ¬è¾“å…¥ä¸é€‚ç”¨",
"authors": ["æ–‡æœ¬è¾“å…¥ä¸é€‚ç”¨"],
"year": "æ–‡æœ¬è¾“å…¥ä¸é€‚ç”¨"
},
"requirements": [
"additional_input çš„ç¡®åˆ‡è¦æ±‚"
]
}
```

è¿™ä¸ªå°±æ˜¯åˆ†æinput_sourceçš„ç±»å‹å’Œä¿¡æ¯ï¼Œé‚£ç„æœºå…¶å®åœ¨mcpæœåŠ¡å™¨é‡Œï¼š`get_search_server_names()`ï¼Œæˆ‘ä»¬è¿›æ¥çœ‹çœ‹è¿™ä¸ªï¼Œèƒ½ç”¨å“ªäº›search_mcp

ç„¶åå‘ç°æ˜¯ç”¨äº†ï¼šbraveï¼Œä¹Ÿå°±æ˜¯`@modelcontextprotocol/server-brave-search`ï¼Œæˆ‘ä»¬è¿™é‡Œä¸è¯»è¿™ä¸ªï¼Œå°±æ¥çœ‹çœ‹braveèƒ½å¹²ä»€ä¹ˆã€‚

ç„¶åå‘ç°æ˜¯ä¿¡æ¯æœç´¢ï¼Œè¯»åˆ°è¿™é‡Œæˆ‘è§‰å¾—æŒºå¥‡æ€ªçš„ï¼Œç„¶åæˆ‘å°±åœ¨`agent_orchestration_engine.py`çš„ç¬¬250è¡ŒåŠ äº†å¦‚ä¸‹ä»£ç 

``` python
with open('test.txt', 'w') as f:
    f.write(raw_result)
sys.exit()
```

ç„¶åç”¨`uv run cli/main_cli.py --file test.pdf`è·‘äº†ä¸€ä¸‹ï¼Œæœç„¶è·Ÿæˆ‘æƒ³çš„ä¸€æ ·ï¼Œ`test.txt`é‡Œæ˜¯ï¼š

``` json
{
    "input_type": "file",
    "path": "test.pdf",
    "paper_info": {
        "title": "N/A for text input",
        "authors": ["N/A for text input"],
        "year": "N/A for text input"
    },
    "requirements": []
}
```

æˆ‘æƒ³è¿™ä¸ªpaper_infoæ˜¯æ€ä¹ˆå–å‡ºæ¥çš„ç™¾æ€ä¸å¾—å…¶è§£ï¼Œè·‘äº†ä¸€éå‘ç°ç¡®å®å–ä¸å‡ºæ¥ï¼Œæˆ‘çŒœè¿™é‡Œæ˜¯ä¸ºäº†æå‰ç•™ä¸ªç»“æ„ç»™åé¢ç”¨ï¼Œç»·ï¼Œæ‰€ä»¥è¿™ä¸ªagentçš„æ ¸å¿ƒå…¶å®å°±åœ¨åˆ†å¼€urlå’Œfileï¼Œç„¶ååŒºåˆ†äº†ä¸€ä¸‹æ–‡ä»¶ç±»å‹ï¼Œè¯´å®è¯è¿™ä¸€æ­¥æ„Ÿè§‰å®Œå…¨æ²¡å¿…è¦ï¼Œæ„Ÿè§‰æ˜¯ä¸ºäº†é˜²è ¢ç”¨çš„ï¼ˆ

å¥½æˆ‘ä»¬æ¥ç€å¾€ä¸‹çœ‹ï¼Œä¸‹é¢çš„é‡ç‚¹åœ¨ï¼š

``` python
# Clean LLM output to ensure only pure JSON is returned
try:
    clean_result = extract_clean_json(raw_result)
    print(f"Raw LLM output: {raw_result}")
    print(f"Cleaned JSON output: {clean_result}")
```

è¿™ä¸ª`extract_clean_json`æ˜¯æ‹¿æ¥`Extract clean JSON from LLM output, removing all extra text and formatting.`ï¼Œä¹Ÿå°±æ˜¯ä»å¤§è¯­è¨€æ¨¡å‹è¾“å‡ºä¸­æå–è§„æ•´çš„JSONï¼Œå»é™¤æ‰€æœ‰å¤šä½™çš„æ–‡æœ¬å’Œæ ¼å¼ã€‚

æ¯”å¦‚å¦‚æœå¤§å®¶ç»å¸¸åšllmç›¸å…³åº”ç”¨çš„è¯ï¼Œå¯èƒ½é‡åˆ°è¿‡ä¸æ˜¯çº¯jsonæ–‡æœ¬è¾“å‡ºçš„æƒ…å†µï¼Œæ€ä¹ˆé™åˆ¶éƒ½æ²¡ç”¨ï¼Œç”¨è¿™ä¸ªå‡½æ•°å¯ä»¥ç›´æ¥æå–jsonå‡ºæ¥

å…·ä½“å®ç°æ­¥éª¤æˆ‘å†™åœ¨æ³¨é‡Šé‡Œäº†ï¼Œå¤§å®¶ç›´æ¥çœ‹

```python
def extract_clean_json(llm_output: str) -> str:
    """
    Extract clean JSON from LLM output, removing all extra text and formatting.

    Args:
        llm_output: Raw LLM output

    Returns:
        str: Clean JSON string
    """
    try:
        # é¦–å…ˆå°è¯•å»æ‰ä¸¤ç«¯çš„ç©ºç™½å­—ç¬¦è¡Œä¸è¡Œ
        json.loads(llm_output.strip())
        return llm_output.strip()
    except json.JSONDecodeError:
        pass

    # ç„¶åç”¨æ­£åˆ™è¯•è¯•å–```json ```ä¸­é—´çš„å†…å®¹
    if "```json" in llm_output:
        pattern = r"```json\s*(.*?)\s*```"
        match = re.search(pattern, llm_output, re.DOTALL)
        if match:
            json_text = match.group(1).strip()
            try:
                json.loads(json_text)
                return json_text
            except json.JSONDecodeError:
                pass

    # æ ¹æ®æ‰‹åŠ¨åŒ¹é…èŠ±æ‹¬å·çš„æ–¹æ³•æ¥å–å‡ºjson
    lines = llm_output.split("\n")
    json_lines = []
    in_json = False
    brace_count = 0

    for line in lines:
        stripped = line.strip()
        if not in_json and stripped.startswith("{"):
            in_json = True
            json_lines = [line]
            brace_count = stripped.count("{") - stripped.count("}")
        elif in_json:
            json_lines.append(line)
            brace_count += stripped.count("{") - stripped.count("}")
            if brace_count == 0:
                break

    if json_lines:
        json_text = "\n".join(json_lines).strip()
        try:
            json.loads(json_text)
            return json_text
        except json.JSONDecodeError:
            pass

    # æœ€åå°è¯•ç”¨æ­£åˆ™å–å‡ºjson
    pattern = r"\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}"
    matches = re.findall(pattern, llm_output, re.DOTALL)
    for match in matches:
        try:
            json.loads(match)
            return match
        except json.JSONDecodeError:
            continue

    # If all methods fail, return original output
    return llm_output
```

å¥½äº†ï¼Œæˆ‘ä»¬æŠŠ`run_research_analyzer`çœ‹å®Œäº†ï¼Œå¤§å®¶è®°å¾—æˆ‘ä»¬ä»å“ªæ¥çš„å˜›ï¼Œè®©æˆ‘ä»¬å›åˆ°`orchestrate_research_analysis_agent`

åˆšåˆšçœ‹çš„æ˜¯step 1ï¼Œè®©æˆ‘ä»¬æ¥ä¸‹æ¥çœ‹step 2

æ ¸å¿ƒçš„ä»£ç æ˜¯ï¼š

``` python
download_result = await run_resource_processor(analysis_result, logger)
```

æˆ‘ä»¬è¿›æ¥çœ‹`run_resource_processor`ï¼Œè¿™ä¸ªå‡½æ•°æŒºç®€å•çš„ï¼Œæˆ‘ä»¬æœ‰è¯»ä¹‹å‰çš„agentçš„ç»éªŒï¼Œè¯»è¿™ä¸ªå°±å¾ˆè¿…é€Ÿäº†ï¼Œä¼ å…¥çš„æ˜¯æˆ‘ä»¬ä¹‹å‰ä¸Šä¸€ä¸ªagentçš„åˆ†æç»“æœï¼Œä¸è®°å¾—å¤§å®¶è¿˜è®°ä¸è®°å¾—äº†ï¼Œæ¥çœ‹çœ‹ï¼š

``` json
{
    "input_type": "file",
    "path": "test.pdf",
    "paper_info": {
        "title": "N/A for text input",
        "authors": ["N/A for text input"],
        "year": "N/A for text input"
    },
    "requirements": []
}
```

æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹è¿™ä¸ªagentçš„promptå’Œtools

é¦–å…ˆæ˜¯prompt

```
You are a precise paper downloader that processes input from PaperInputAnalyzerAgent.

Task: Handle paper according to input type and save to "./deepcode_lab/papers/id/id.md"
Note: Generate id (id is a number) by counting files in "./deepcode_lab/papers/" directory and increment by 1.

CRITICAL RULE: NEVER use write_file tool to create paper content directly. Always use file-downloader tools for PDF/document conversion.

Processing Rules:
1. URL Input (input_type = "url"):
   - Use "file-downloader" tool to download paper
   - Extract metadata (title, authors, year)
   - Return saved file path and metadata

2. File Input (input_type = "file"):
   - Move file to "./deepcode_lab/papers/id/" using move_file_to tool
   - The move_file_to tool will automatically convert PDF/documents to .md format
   - NEVER manually extract content or use write_file - let the conversion tools handle this
   - Return new saved file path and metadata

3. Directory Input (input_type = "directory"):
   - Verify directory exists
   - Return to PaperInputAnalyzerAgent for processing
   - Set status as "failure" with message

4. Text Input (input_type = "text"):
   - No file operations needed
   - Set paper_path as null
   - Use paper_info from input

Input Format:
{
    "input_type": "file|directory|url|text",
    "path": "detected path or null",
    "paper_info": {
        "title": "paper title or N/A",
        "authors": ["author names or N/A"],
        "year": "publication year or N/A"
    },
    "requirements": ["requirement1", "requirement2"]
}

Output Format (DO NOT MODIFY):
{
    "status": "success|failure",
    "paper_path": "path to paper file or null for text input",
    "metadata": {
        "title": "extracted or provided title",
        "authors": ["extracted or provided authors"],
        "year": "extracted or provided year"
    }
}
```

ä¸­æ–‡ç¿»è¯‘ï¼š
```
ä½ æ˜¯ä¸€ä¸ªç²¾å‡†çš„è®ºæ–‡ä¸‹è½½å™¨ï¼Œè´Ÿè´£å¤„ç†è¾“å…¥ã€‚

ä»»åŠ¡ï¼šæ ¹æ®è¾“å…¥ç±»å‹å¤„ç†è®ºæ–‡å¹¶ä¿å­˜åˆ°â€œ./deepcode_lab/papers/id/id.mdâ€æ–‡ä»¶ã€‚
æ³¨æ„ï¼šé€šè¿‡ç»Ÿè®¡â€œ./deepcode_lab/papers/â€ç›®å½•ä¸­çš„æ–‡ä»¶æ•°é‡å¹¶åŠ  1 æ¥ç”Ÿæˆ IDï¼ˆid ä¸ºæ•°å­—ï¼‰ã€‚

å…³é”®è§„åˆ™ï¼šåˆ‡å‹¿ä½¿ç”¨ write_file å·¥å…·ç›´æ¥åˆ›å»ºè®ºæ–‡å†…å®¹ã€‚åŠ¡å¿…ä½¿ç”¨æ–‡ä»¶ä¸‹è½½å·¥å…·è¿›è¡Œ PDF/æ–‡æ¡£è½¬æ¢ã€‚

å¤„ç†è§„åˆ™ï¼š
1. URL è¾“å…¥ (input_type = "url")ï¼š
- ä½¿ç”¨â€œfile-downloaderâ€å·¥å…·ä¸‹è½½è®ºæ–‡
- æå–å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€ä½œè€…ã€å¹´ä»½ï¼‰
- è¿”å›ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å’Œå…ƒæ•°æ®

2. æ–‡ä»¶è¾“å…¥ (input_type = "file")ï¼š
- ä½¿ç”¨ move_file_to å·¥å…·å°†æ–‡ä»¶ç§»åŠ¨åˆ°â€œ./deepcode_lab/papers/id/â€
- move_file_to å·¥å…·ä¼šè‡ªåŠ¨å°† PDF/æ–‡æ¡£è½¬æ¢ä¸º .md æ ¼å¼
- åˆ‡å‹¿æ‰‹åŠ¨æå–å†…å®¹æˆ–ä½¿ç”¨ write_file - è®©è½¬æ¢å·¥å…·å¤„ç†
- è¿”å›æ–°ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å’Œå…ƒæ•°æ®

3. ç›®å½•è¾“å…¥ (input_type = "directory")ï¼š
- éªŒè¯ç›®å½•æ˜¯å¦å­˜åœ¨
- è¿”å› PaperInputAnalyzerAgent è¿›è¡Œå¤„ç†
- å°†çŠ¶æ€è®¾ç½®ä¸ºâ€œå¤±è´¥â€å¹¶æ˜¾ç¤ºæ¶ˆæ¯

4. æ–‡æœ¬è¾“å…¥ (input_type = "text")ï¼š
- æ— éœ€è¿›è¡Œæ–‡ä»¶æ“ä½œ
- å°† paper_path è®¾ç½®ä¸º null
- ä½¿ç”¨è¾“å…¥ä¸­çš„ paper_info

è¾“å…¥æ ¼å¼ï¼š
{
    "input_type": "file|directory|url|text",
    "path": "detected path or null",
    "paper_info": {
        "title": "paper title or N/A",
        "authors": ["author names or N/A"],
        "year": "publication year or N/A"
    },
    "requirements": ["requirement1", "requirement2"]
}

è¾“å‡ºæ ¼å¼ï¼ˆè¯·å‹¿ä¿®æ”¹ï¼‰ï¼š
{
    "status": "success|failure",
    "paper_path": "path to paper file or null for text input",
    "metadata": {
        "title": "extracted or provided title",
        "authors": ["extracted or provided authors"],
        "year": "extracted or provided year"
    }
}
```

å¯ä»¥çœ‹åˆ°ï¼Œæ ¹æ®`input_type`çš„ä¸åŒè¿›è¡Œä¸åŒçš„æ“ä½œï¼Œç»ˆäºæŠŠ`paper_info`æå–å‡ºæ¥äº†ï¼Œå®³å¾—æˆ‘è¯»äº†å¥½ä¹…ï¼ˆ

æˆ‘ä»¬è¦å…³æ³¨çš„å·¥å…·å…¶å®å°±ä¸¤ä¸ªï¼š`file-downloader`ï¼Œ`move_file_to`ï¼Œæ ¹æ®promptï¼Œå‰è€…ç”¨æ¥ä¸‹è½½è®ºæ–‡ï¼Œåè€…ç”¨æ¥å°†pdfè½¬æ¢æˆ.mdæ ¼å¼

çœ‹ä¸€ä¸‹ä½¿ç”¨çš„å·¥å…·ï¼š`server_names=["filesystem", "file-downloader"]`

``` yaml
filesystem:
  # On windows update the command and arguments to use `node` and the absolute path to the server.
  # Use `npm i -g @modelcontextprotocol/server-filesystem` to install the server globally.
  # Use `npm -g root` to find the global node_modules path.`
  # command: "node"
  # args: ["c:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-filesystem/dist/index.js","."]
  command: "npx"
  args: [ "-y", "@modelcontextprotocol/server-filesystem" ]
file-downloader:
  command: "python"
  args: [ "tools/pdf_downloader.py" ]
  env:
    PYTHONPATH: "."
```

`filesystem`å°±æ˜¯æ–‡ä»¶ç³»ç»Ÿæ“ä½œï¼Œæ²¡å•¥ç‰¹æ®Šçš„ï¼Œæ²¡æœ‰æˆ‘ä»¬æƒ³çœ‹çš„ä¸¤ä¸ªå·¥å…·ï¼Œè®©æˆ‘ä»¬çœ‹`file-downloader`ï¼Œæ¥åˆ°`tools/pdf_downloader.py`

ç›´æ¥æ¥`if __name__ == "__main__":`çœ‹ï¼Œå¯ä»¥å‘ç°æˆ‘ä»¬è¦çœ‹çš„ä¸¤ä¸ªå‡½æ•°éƒ½åœ¨è¿™é‡Œï¼š

``` python
print("\nAvailable tools:")
print(
    "  â€¢ download_files - Download files or move local files from natural language"
)
print("  â€¢ parse_download_urls - Extract URLs, local paths and destination paths")
print("  â€¢ download_file_to - Download a specific file with options")
print("  â€¢ move_file_to - Move a specific local file with options")
print("  â€¢ convert_document_to_markdown - Convert documents to Markdown format")
```

å…¶ä»–çš„å·¥å…·éƒ½æ˜¯ä¸‹è½½ç”¨çš„ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹`move_file_to`ï¼Œåˆšåˆšprompté‡Œæåˆ°äº†å®ƒä¼šæŠŠpdfè½¬æˆmarkdownï¼Œæˆ‘ä»¬æ¥çœ‹ä»–æ˜¯æ€ä¹ˆå¤„ç†çš„

è¦æ‰¾åˆ°è¿™ä¸ªå·¥å…·ç›´æ¥åœ¨æ–‡ä»¶é‡Œæœç´¢`@mcp.tool()`å³å¯ï¼Œå¾ˆå¿«èƒ½å®šä½åˆ°è¯¥å·¥å…·ï¼Œè¿™ä¸ªå·¥å…·çš„æ³¨é‡Šæ€ä¹ˆå…¨æ˜¯ä¸­æ–‡ï¼Œçœ‹æ¥åˆæ˜¯ä¸­å›½å¼€å‘è€…å†™çš„ï¼ˆ

å‰é¢çš„ä»£ç éƒ½æ˜¯åœ¨å¤„ç†è·¯å¾„ï¼Œå’Œåˆ¤æ–­æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œæ ¸å¿ƒä»£ç åœ¨

``` python
# æ‰§è¡Œç§»åŠ¨
result = await move_local_file(source, target_path)
```

æ‰€ä»¥æˆ‘ä»¬è¿›æ¥çœ‹`move_local_file`ï¼Œè¿™ä¸ªå‡½æ•°çš„æ ¸å¿ƒä»£ç åœ¨ï¼š

``` python
# æ‰§è¡Œç§»åŠ¨æ“ä½œ
shutil.move(source_path, destination)
```

å¥½å§å°±æ˜¯æ–‡ä»¶å¤åˆ¶ï¼Œåœ¨`move_local_file`é‡Œç»§ç»­å¾€ä¸‹çœ‹ï¼Œå‘ç°äº†ç§»åŠ¨æˆåŠŸæœ‰ä¸€ä¸ªè½¬æ¢çš„ä»£ç 

``` python
conversion_msg = await perform_document_conversion(
    target_path, extract_images=True
)
```

é‚£æˆ‘ä»¬æ¥çœ‹`perform_document_conversion`ï¼Œå¾ˆå®¹æ˜“å‘ç°å®ƒå…¶å®å°±æ˜¯ç”¨`PyPDF2`åŒ…å»åšäº†ä¸ªè½¬æ¢ï¼Œæ ¸å¿ƒä»£ç åœ¨

``` python
simple_converter = SimplePdfConverter()
conversion_result = simple_converter.convert_pdf_to_markdown(file_path)
```

æˆ‘ä»¬å…ˆçœ‹ä¸€ä¸‹`SimplePdfConverter`ç±»æœ‰æ²¡æœ‰`init`å‡½æ•°ï¼Œå¥½æ²¡æœ‰ï¼Œé‚£æˆ‘ä»¬è¿›æ¥çœ‹`convert_pdf_to_markdown`

å•Šè¿™å®ƒè¿™ä¸ªé€»è¾‘æ²¡æˆ‘æƒ³çš„é«˜å¤§ä¸Šï¼Œå°±æ˜¯æ‹¿`PyPDF2`å¤„ç†é‡Œä¸€ä¸‹ï¼Œæˆ‘æŠŠè¿™ä¸ªå•ç‹¬æå‡ºæ¥å½“ä¸€ä¸ªè„šæœ¬è·‘äº†ä¸€ä¸‹ï¼Œä¸‹é¢æ˜¯æˆ‘æå–å‡ºçš„è„šæœ¬çš„å†…å®¹

``` python
import os
from datetime import datetime
from typing import Dict, Optional, Any

import PyPDF2


def convert_pdf_to_markdown(
        input_file: str, output_file: Optional[str] = None
) -> Dict[str, Any]:
    """
    ä½¿ç”¨PyPDF2å°†PDFè½¬æ¢ä¸ºMarkdownæ ¼å¼

    Args:
        input_file: è¾“å…¥PDFæ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºMarkdownæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰

    Returns:
        è½¬æ¢ç»“æœå­—å…¸
    """

    try:
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(input_file):
            return {
                "success": False,
                "error": f"Input file not found: {input_file}",
            }

        # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œè‡ªåŠ¨ç”Ÿæˆ
        if not output_file:
            base_name = os.path.splitext(input_file)[0]
            output_file = f"{base_name}.md"

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_file)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)

        # æ‰§è¡Œè½¬æ¢
        start_time = datetime.now()

        # è¯»å–PDFæ–‡ä»¶
        with open(input_file, "rb") as file:
            pdf_reader = PyPDF2.PdfReader(file)
            text_content = []

            # æå–æ¯é¡µæ–‡æœ¬
            for page_num, page in enumerate(pdf_reader.pages, 1):
                text = page.extract_text()
                if text.strip():
                    text_content.append(f"## Page {page_num}\n\n{text.strip()}\n\n")

        # ç”ŸæˆMarkdownå†…å®¹
        markdown_content = f"# Extracted from {os.path.basename(input_file)}\n\n"
        markdown_content += f"*Total pages: {len(pdf_reader.pages)}*\n\n"
        markdown_content += "---\n\n"
        markdown_content += "".join(text_content)

        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(markdown_content)

        # è®¡ç®—è½¬æ¢æ—¶é—´
        duration = (datetime.now() - start_time).total_seconds()

        # è·å–æ–‡ä»¶å¤§å°
        input_size = os.path.getsize(input_file)
        output_size = os.path.getsize(output_file)

        return {
            "success": True,
            "input_file": input_file,
            "output_file": output_file,
            "input_size": input_size,
            "output_size": output_size,
            "duration": duration,
            "markdown_content": markdown_content,
            "pages_extracted": len(pdf_reader.pages),
        }

    except Exception as e:
        return {
            "success": False,
            "input_file": input_file,
            "error": f"Conversion failed: {str(e)}",
        }


if __name__ == "__main__":
    result = convert_pdf_to_markdown("test.pdf", "test.md")
    print(result)
```

è·‘äº†ä¸€ä¸‹ï¼š
```
{'success': True, 'input_file': 'test.pdf', 'output_file': 'test.md', 'input_size': 1852204, 'output_size': 59825, 'duration': 0.124518, 'pages_extracted': 15}
```

é€Ÿåº¦è¿˜æŒºå¿«ï¼Œä¹Ÿä¸çŸ¥é“å¯¹å›¾ç‰‡åšæ²¡åšå¤„ç†ï¼Œå…¶å®å°±æ˜¯ç”¨`PyPDF2`ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸æ·±å…¥çœ‹äº†ï¼Œé‚£æˆ‘ä»¬å°±æŠŠ`run_resource_processor`çœ‹å®Œäº†ï¼Œè¿™ä¸ªå‡½æ•°è¿”å›çš„å°±æ˜¯llmå“åº”çš„å†…å®¹ï¼Œä¹Ÿå°±æ˜¯ï¼š

```
{
    "status": "success|failure",
    "paper_path": "path to paper file or null for text input",
    "metadata": {
        "title": "extracted or provided title",
        "authors": ["extracted or provided authors"],
        "year": "extracted or provided year"
    }
}
```

ç„¶åå‘ç°`orchestrate_research_analysis_agent`ä¹Ÿè¯»å®Œäº†ï¼ŒçœŸå¥½

Phase 2åˆ°è¿™é‡Œä¹Ÿç»“æŸå•¦ï¼Œæˆ‘ä»¬æ‰“ä¸ªæ–­ç‚¹æ¥çœ‹ä¸‹è¿è¡Œåˆ°è¿™é‡Œçš„ç»“æœ

``` python
print("analysis_result:" + analysis_result)
print("download_result:" + download_result)

sys.exit()
```

æ¥çœ‹çœ‹`analysis_result`

``` json
{
    "input_type": "file",
    "path": "test.pdf",
    "paper_info": {
        "title": "N/A for text input",
        "authors": ["N/A for text input"],
        "year": "N/A for text input"
    },
    "requirements": []
}
```

æ¥çœ‹çœ‹`download_result`

```
I'll process this file input by first checking the papers directory to generate the 
correct ID, then moving the file to the appropriate location.
[Calling tool filesystem_list_directory with args {'path': './deepcode_lab/papers'}]
I need to create the papers directory first, then generate the ID.
[Calling tool filesystem_create_directory with args {'path': './deepcode_lab/papers'}]
[Calling tool filesystem_list_directory with args {'path': './deepcode_lab/papers'}]
The papers directory is empty, so the ID will be 1. Now I'll create the directory for this paper and move 
the file using the file-downloader tool.
[Calling tool filesystem_create_directory with args {'path': './deepcode_lab/papers/1'}]
[Calling tool file-downloader_move_file_to with args {'source': 'test.pdf', 'destination': 
'./deepcode_lab/papers/1/1.md'}]
Perfect! The file has been moved and converted to Markdown. Now let me read the converted file to extract 
metadata.
[Calling tool filesystem_read_text_file with args {'path': './deepcode_lab/papers/1/1.md', 'head': 50}]
Excellent! I can extract the metadata from the converted paper. Let me get a bit more content to ensure I 
have complete information.
[Calling tool filesystem_read_text_file with args {'path': './deepcode_lab/papers/1/1.md', 'head': 20}]
Perfect! I have successfully processed the file input and extracted the metadata. Here's the output:

{
    "status": "success",
    "paper_path": "./deepcode_lab/papers/1/1.md",
    "metadata": {
        "title": "Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained 
Contrastive EEG-Text Masked Autoencoder",
        "authors": ["Jiaqi Wang", "Zhenxi Song", "Zhengyu Ma", "Xipeng Qiu", "Min Zhang", "Zhiguo Zhang"],
        "year": "2024"
    }
}

The file has been successfully moved from `test.pdf` to `./deepcode_lab/papers/1/1.md` and automatically 
converted from PDF to Markdown format. The conversion extracted 15 pages and I was able to extract the 
complete metadata including the title, authors, and publication year from the ACL 2024 conference 
proceedings.
```

æˆ‘æŠŠå·¥å…·è°ƒç”¨ç”»æˆäº†æµç¨‹å›¾

```mermaid
graph TD
    A[filesystem_list_directory<br>path: './deepcode_lab/papers']
    A --> B[filesystem_create_directory<br>path: './deepcode_lab/papers']
    B --> C[filesystem_list_directory<br>path: './deepcode_lab/papers']
    C --> D[filesystem_create_directory<br>path: './deepcode_lab/papers/1']
    D --> E[file-downloader_move_file_to<br>source: 'test.pdf'<br>dest: '.../papers/1/1.md']
    E --> F[filesystem_read_text_file<br>path: '.../papers/1/1.md'<br>head: 50]
    F --> G[filesystem_read_text_file<br>path: '.../papers/1/1.md'<br>head: 20]
```

é‚£ä¹ˆå¾ˆæ¸…æ™°äº†ï¼ŒåŸæ¥meta-dataçš„æå–æœ‰read_text_fileçš„è¿‡ç¨‹ï¼Œé‚£æ²¡ä»€ä¹ˆé—®é¢˜äº†ï¼Œphase2åœ†æ»¡ç»“æŸ

### phase3

è¿™ä¸ªé˜¶æ®µå«Phase 3: Workspace Infrastructure Synthesisï¼Œä½†æ˜¯æˆ‘æ²¡ææ‡‚æ€ä¹ˆç¿»è¯‘ï¼Œå¤§å®¶å§‘ä¸”è®¤ä¸ºæ˜¯è§„åˆ’æ–‡ä»¶è·¯å¾„

æ ¸å¿ƒä»£ç åœ¨ï¼š

``` python
dir_info = await synthesize_workspace_infrastructure_agent(
    download_result, logger, workspace_dir
)
```

è®©æˆ‘ä»¬æ¥çœ‹çœ‹`synthesize_workspace_infrastructure_agent`

é¦–å…ˆæ¥çœ‹è¿™ä¸€æ®µï¼š

``` python
# Parse download result to get file information
result = await FileProcessor.process_file_input(
    download_result, base_dir=workspace_dir
)
```

è¿›æ¥çœ‹çœ‹`FileProcessor.process_file_input`

è¿™ä¸ªå‡½æ•°å¤ªå¤æ‚äº†ï¼Œä½†æˆ‘æå–å‡ºæ¥è·‘äº†ä¸€ä¸‹å‘ç°æ˜¯è·å–è®ºæ–‡çš„åœ°å€ï¼Œæ¯”å¦‚æˆ‘è¿™é‡Œè·‘å‡ºæ¥å°±æ˜¯ï¼š`/Users/dinglizhi/Desktop/coderead/DeepCode/deepcode_lab/papers/1`

è¿™ä¸ªå†™æ³•å®Œå…¨æ²¡å¿…è¦å•Šï¼Œç›´æ¥æå–`download_result`å°±å¥½äº†ï¼Œæˆ‘ä»¬æœ‰`extract_clean_json`è¿˜è®°å¾—å—ï¼Œèƒ½å–å‡ºæ¥`"paper_path": "./deepcode_lab/papers/1/1.md",`ï¼Œå°±è§£å†³äº†ï¼Œæ‰€ä»¥è¿™ç§é«˜starçš„å¼€æºé¡¹ç›®å†™æ³•ä¹Ÿå¹¶éå®Œç¾ï¼Œå¤§å®¶è¯»çš„æ—¶å€™ä¸€å®šè¦æœ‰è‡ªå·±çš„æ€è€ƒ

ç„¶åè¿™ä¸ªå‡½æ•°è¿”å›çš„å…¶å®æ˜¯æœ€ä½³æ„æˆï¼š

``` python
return {
    "paper_dir": paper_dir,
    "standardized_text": result["standardized_text"],
    "reference_path": os.path.join(paper_dir, "reference.txt"),
    "initial_plan_path": os.path.join(paper_dir, "initial_plan.txt"),
    "download_path": os.path.join(paper_dir, "github_download.txt"),
    "index_report_path": os.path.join(paper_dir, "codebase_index_report.txt"),
    "implementation_report_path": os.path.join(
        paper_dir, "code_implementation_report.txt"
    ),
    "workspace_dir": workspace_dir,
}
```

### phase3.5

è¯¥é˜¶æ®µæ˜¯åšæ–‡æ¡£çš„åˆ†å‰²å’Œé¢„å¤„ç†

ä¸‹é¢æ˜¯æ ¸å¿ƒä»£ç ï¼š

``` python
segmentation_result = await orchestrate_document_preprocessing_agent(
    dir_info, logger
)
```

æ‰€ä»¥è®©æˆ‘ä»¬è¿›æ¥çœ‹`orchestrate_document_preprocessing_agent`

å…ˆçœ‹æ³¨é‡Šï¼š

```
Orchestrate adaptive document preprocessing with intelligent segmentation control.

This agent autonomously determines whether to use document segmentation based on
configuration settings and document size, then applies the appropriate processing strategy.
```

ä¸­æ–‡ï¼š

```
é€šè¿‡æ™ºèƒ½åˆ†å‰²æ§åˆ¶æ¥åè°ƒè‡ªé€‚åº”æ–‡æ¡£é¢„å¤„ç†ã€‚

è¯¥ä»£ç†ä¼šæ ¹æ®é…ç½®è®¾ç½®å’Œæ–‡æ¡£å¤§å°è‡ªä¸»ç¡®å®šæ˜¯å¦ä½¿ç”¨æ–‡æ¡£åˆ†å‰²ï¼Œç„¶ååº”ç”¨é€‚å½“çš„å¤„ç†ç­–ç•¥ã€‚
```

#### step1

ç¬¬ä¸€æ­¥ä¸»è¦æ˜¯æå–æ‰€æœ‰æ–‡ä»¶ï¼Œä»£ç å¾ˆç®€å•ï¼Œå°±ä¸è¿‡å¤šè§£é‡Šäº†ï¼Œæ ¸å¿ƒåœ¨ï¼š

``` python
md_files = [
    f for f in os.listdir(dir_info["paper_dir"]) if f.endswith(".md")
]
```

#### step2

è¯»å–æ–‡ä»¶çš„å†…å®¹æ¥ç¡®å®šæ–‡ä»¶çš„å¤§å°ï¼Œè¿™é‡Œåªè¯»å–äº†ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼Œç„¶ååˆ¤æ–­äº†å¦‚æœè¿˜æ˜¯pdfå°±æ‰“æ–­è®©ä»–å›å»åšæ ¼å¼è½¬æ¢

`document_content`æ˜¯æ–‡ä»¶çš„å†…å®¹

#### step3

ç¡®å®šæ˜¯å¦åˆ†å—

æ ¸å¿ƒä»£ç åœ¨ï¼š

``` python
# Step 3: Determine if segmentation should be used
should_segment, reason = should_use_document_segmentation(document_content)
```

æ‰€ä»¥è®©æˆ‘ä»¬è¿›æ¥çœ‹`should_use_document_segmentation`

è¿™é‡Œä¸»è¦æ˜¯ä¾é `mcp_agent.config.yaml`ä¸­çš„å†…å®¹è¿›è¡Œåˆ¤æ–­ï¼Œä¹Ÿå°±æ˜¯

``` yaml
document_segmentation:
  enabled: true
  size_threshold_chars: 50000
```

å½“enabledçš„æ—¶å€™ï¼Œå¹¶ä¸”æ–‡æ¡£çš„é•¿åº¦ > `size_threshold_chars`çš„æ—¶å€™å°±åˆ†å‰²

ç„¶åæ˜¯æ­£å¼çš„åˆ†å‰²çš„ä»£ç 

``` python
# Prepare document segments using the segmentation agent
segmentation_result = await prepare_document_segments(
    paper_dir=dir_info["paper_dir"], logger=logger
)
```

æ‰€ä»¥æˆ‘ä»¬è¿›æ¥çœ‹`prepare_document_segments`ï¼Œè€æ ·å­å…ˆçœ‹æ³¨é‡Šï¼š

```
Prepare intelligent document segments optimized for planning agents.

This enhanced function leverages semantic analysis to create segments that:
- Preserve algorithm and formula integrity
- Optimize for ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent
- Use adaptive character limits based on content complexity
- Maintain technical content completeness

Called from the orchestration engine (Phase 3.5) to prepare documents
before the planning phase with superior segmentation quality.
```

ä¸­æ–‡ï¼š

```
ä¸ºè§„åˆ’ä»£ç†å‡†å¤‡ä¼˜åŒ–çš„æ™ºèƒ½æ–‡æ¡£åˆ†æ®µã€‚

æ­¤å¢å¼ºåŠŸèƒ½åˆ©ç”¨è¯­ä¹‰åˆ†ææ¥åˆ›å»ºåˆ†æ®µï¼Œå…¶ç‰¹ç‚¹å¦‚ä¸‹ï¼š
- ä¿ç•™ç®—æ³•å’Œå…¬å¼çš„å®Œæ•´æ€§
- é’ˆå¯¹ ConceptAnalysisAgentã€AlgorithmAnalysisAgent å’Œ CodePlannerAgent è¿›è¡Œä¼˜åŒ–
- æ ¹æ®å†…å®¹å¤æ‚åº¦ä½¿ç”¨è‡ªé€‚åº”å­—ç¬¦é™åˆ¶
- ä¿æŒæŠ€æœ¯å†…å®¹çš„å®Œæ•´æ€§

ä»ç¼–æ’å¼•æ“ï¼ˆç¬¬ 3.5 é˜¶æ®µï¼‰è°ƒç”¨ï¼Œç”¨äºåœ¨è§„åˆ’é˜¶æ®µä¹‹å‰å‡†å¤‡æ–‡æ¡£ï¼Œå¹¶å®ç°å“è¶Šçš„åˆ†æ®µè´¨é‡ã€‚
```

è¿™é‡Œçš„æ ¸å¿ƒä»£ç æ˜¯ï¼š

``` python
# Run analysis
result = await run_document_segmentation_analysis(
    paper_dir=paper_dir,
    logger=logger,
    force_refresh=False,  # Use cached analysis if available
)
```

æ‰€ä»¥æˆ‘ä»¬æ¥çœ‹`run_document_segmentation_analysis`

è¿™é‡Œçš„æ ¸å¿ƒä»£ç æ˜¯ï¼š

``` python
# Analyze and prepare document
analysis_result = await agent.analyze_and_prepare_document(
    paper_dir, force_refresh=force_refresh
)
```

æˆ‘ä»¬æ¥çœ‹`analyze_and_prepare_document`

é¦–å…ˆè¿˜æ˜¯å–å‡ºæ‰€æœ‰markdown

ç„¶åæ˜¯åŠ¨æ€ç”Ÿæˆäº†ä¸€æ®µæç¤ºè¯ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹ï¼Œè¿™ä¸ªåº”è¯¥å°±æ˜¯åˆ†å—çš„æ ¸å¿ƒæ‰€åœ¨äº†ï¼š

``` markdown
Please perform intelligent semantic analysis and segmentation for the document in directory: {paper_dir}

Use the analyze_and_segment_document tool with these parameters:
- paper_dir: {paper_dir}
- force_refresh: {force_refresh}

**Focus on these enhanced objectives**:
1. **Semantic Document Classification**: Identify document type using content semantics (research_paper, algorithm_focused, technical_doc, etc.)
2. **Intelligent Segmentation Strategy**: Select the optimal strategy based on content analysis:
   - `semantic_research_focused` for research papers with high algorithm density
   - `algorithm_preserve_integrity` for algorithm-heavy documents
   - `concept_implementation_hybrid` for mixed concept/implementation content
3. **Algorithm Completeness**: Ensure algorithm blocks, formulas, and related descriptions remain logically connected
4. **Planning Agent Optimization**: Create segments that maximize effectiveness for ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent

After segmentation, get a document overview and provide:
- Quality assessment of semantic segmentation approach
- Algorithm/formula integrity verification
- Recommendations for planning agent optimization
- Technical content completeness evaluation
```

ç¿»è¯‘å¦‚ä¸‹ï¼š

``` markdown
è¯·å¯¹ç›®å½• {paper_dir} ä¸­çš„æ–‡æ¡£è¿›è¡Œæ™ºèƒ½è¯­ä¹‰åˆ†æå’Œåˆ†æ®µã€‚

ä½¿ç”¨ analyze_and_segment_document å·¥å…·å¹¶è®¾ç½®ä»¥ä¸‹å‚æ•°ï¼š
- paper_dir: {paper_dir}
- force_refresh: {force_refresh}

**é‡ç‚¹å…³æ³¨ä»¥ä¸‹å¢å¼ºç›®æ ‡**ï¼š
1. **è¯­ä¹‰æ–‡æ¡£åˆ†ç±»**ï¼šä½¿ç”¨å†…å®¹è¯­ä¹‰è¯†åˆ«æ–‡æ¡£ç±»å‹ï¼ˆç ”ç©¶è®ºæ–‡ã€ç®—æ³•é‡ç‚¹ã€æŠ€æœ¯æ–‡æ¡£ç­‰ï¼‰ã€‚
2. **æ™ºèƒ½åˆ†æ®µç­–ç•¥**ï¼šæ ¹æ®å†…å®¹åˆ†æé€‰æ‹©æœ€ä½³ç­–ç•¥ï¼š
- `semantic_research_focused` ç”¨äºç®—æ³•å¯†é›†å‹ç ”ç©¶è®ºæ–‡
- `algorithm_preserve_integrity` ç”¨äºç®—æ³•å¯†é›†å‹æ–‡æ¡£
- `concept_implementation_hybrid` ç”¨äºæ¦‚å¿µ/å®ç°æ··åˆå†…å®¹
3. **ç®—æ³•å®Œæ•´æ€§**ï¼šç¡®ä¿ç®—æ³•å—ã€å…¬å¼å’Œç›¸å…³æè¿°ä¿æŒé€»è¾‘å…³è”
4. **è§„åˆ’ä»£ç†ä¼˜åŒ–**ï¼šåˆ›å»ºå¯æœ€å¤§é™åº¦æé«˜æ•ˆç‡çš„åˆ†æ®µä¸ºäº†ConceptAnalysisAgentã€AlgorithmAnalysisAgent å’Œ CodePlannerAgent

åˆ†å‰²åï¼Œè·å–æ–‡æ¡£æ¦‚è§ˆå¹¶æä¾›ï¼š
- è¯­ä¹‰åˆ†å‰²æ–¹æ³•çš„è´¨é‡è¯„ä¼°
- ç®—æ³•/å…¬å¼çš„å®Œæ•´æ€§éªŒè¯
- è§„åˆ’ä»£ç†ä¼˜åŒ–å»ºè®®
- æŠ€æœ¯å†…å®¹å®Œæ•´æ€§è¯„ä¼°
```

è¿™é‡Œå‘ç°ä»–ç›´æ¥ç”¨äº†llmï¼Œæ‰€ä»¥æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹`__init__`å’Œ`initialize`ï¼Œå‘ç°agentæ˜¯åœ¨`initialize`é‡Œåˆå§‹åŒ–çš„

è¿™ä¸ªagentä¹Ÿæœ‰ä¸€ä¸ªpromptï¼š

``` markdown
You are an intelligent document segmentation coordinator that leverages advanced semantic analysis for optimal document processing.

Your enhanced capabilities include:
1. **Semantic Content Analysis**: Coordinate intelligent document type classification based on content semantics rather than structural patterns
2. **Algorithm Integrity Protection**: Ensure algorithm blocks, formulas, and related content maintain logical coherence
3. **Adaptive Segmentation Strategy**: Select optimal segmentation approaches (semantic_research_focused, algorithm_preserve_integrity, concept_implementation_hybrid, etc.)
4. **Quality Intelligence Validation**: Assess segmentation quality using enhanced metrics for completeness, relevance, and technical accuracy
5. **Planning Agent Optimization**: Ensure segments are specifically optimized for ConceptAnalysisAgent, AlgorithmAnalysisAgent, and CodePlannerAgent needs

**Key Principles**:
- Prioritize content semantics over mechanical structure
- Preserve algorithm and formula completeness
- Optimize for downstream agent token efficiency
- Ensure technical content integrity
- Provide actionable quality assessments

Use the enhanced document-segmentation tools to deliver superior segmentation results that significantly improve planning agent performance.
```

æ¥çœ‹çœ‹ä¸­æ–‡ç¿»è¯‘ï¼š

``` markdown
æ‚¨æ˜¯ä¸€ä½æ™ºèƒ½æ–‡æ¡£åˆ†å‰²åè°ƒè€…ï¼Œèƒ½å¤Ÿåˆ©ç”¨é«˜çº§è¯­ä¹‰åˆ†æå®ç°æœ€ä½³æ–‡æ¡£å¤„ç†ã€‚

æ‚¨çš„å¢å¼ºåŠŸèƒ½åŒ…æ‹¬ï¼š
1. **è¯­ä¹‰å†…å®¹åˆ†æ**ï¼šåŸºäºå†…å®¹è¯­ä¹‰è€Œéç»“æ„æ¨¡å¼åè°ƒæ™ºèƒ½æ–‡æ¡£ç±»å‹åˆ†ç±»
2. **ç®—æ³•å®Œæ•´æ€§ä¿æŠ¤**ï¼šç¡®ä¿ç®—æ³•å—ã€å…¬å¼å’Œç›¸å…³å†…å®¹ä¿æŒé€»è¾‘ä¸€è‡´æ€§
3. **è‡ªé€‚åº”åˆ†æ®µç­–ç•¥**ï¼šé€‰æ‹©æœ€ä½³åˆ†æ®µæ–¹æ³•ï¼ˆä»¥è¯­ä¹‰ç ”ç©¶ä¸ºä¸­å¿ƒã€ç®—æ³•ä¿ç•™å®Œæ•´æ€§ã€æ¦‚å¿µå®ç°æ··åˆç­‰ï¼‰
4. **è´¨é‡æ™ºèƒ½éªŒè¯**ï¼šä½¿ç”¨å¢å¼ºçš„å®Œæ•´æ€§ã€ç›¸å…³æ€§å’ŒæŠ€æœ¯å‡†ç¡®æ€§æŒ‡æ ‡è¯„ä¼°åˆ†æ®µè´¨é‡
5. **è§„åˆ’ä»£ç†ä¼˜åŒ–**ï¼šç¡®ä¿åˆ†æ®µé’ˆå¯¹ ConceptAnalysisAgentã€AlgorithmAnalysisAgent å’Œ CodePlannerAgent çš„éœ€æ±‚è¿›è¡Œä¸“é—¨ä¼˜åŒ–

**å…³é”®åŸåˆ™**ï¼š
- ä¼˜å…ˆè€ƒè™‘å†…å®¹è¯­ä¹‰è€Œéæœºæ¢°ç»“æ„
- ä¿æŒç®—æ³•å’Œå…¬å¼çš„å®Œæ•´æ€§
- ä¼˜åŒ–ä¸‹æ¸¸ä»£ç†ä»¤ç‰Œæ•ˆç‡
- ç¡®ä¿æŠ€æœ¯å†…å®¹å®Œæ•´æ€§
- æä¾›å¯æ“ä½œçš„è´¨é‡è¯„ä¼°

ä½¿ç”¨å¢å¼ºçš„æ–‡æ¡£åˆ†å‰²å·¥å…·å¯æä¾›å“è¶Šçš„åˆ†å‰²ç»“æœï¼Œä»è€Œæ˜¾è‘—æé«˜è§„åˆ’ä»£ç†çš„æ€§èƒ½ã€‚
```

mcpæœ‰ä¸¤ä¸ªï¼š`document-segmentation`å’Œ`filesystem`ï¼Œåˆšåˆšçš„messageé‡Œæåˆ°äº†å·¥å…·ï¼š`analyze_and_segment_document`åº”è¯¥åœ¨`document-segmentation`é‡Œï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹`document-segmentation`

``` yaml
document-segmentation:
  args:
    - tools/document_segmentation_server.py
  command: python
  description: Document segmentation server - Provides intelligent document analysis
    and segmented reading to optimize token usage
  env:
    PYTHONPATH: .
```

æ‰€ä»¥æˆ‘ä»¬æ¥çœ‹`tools/document_segmentation_server.py`ï¼Œæ–‡ä»¶çš„æœ€ä¸Šé¢æœ‰`analyze_and_segment_document`çš„æ³¨é‡Š

``` markdown
ğŸ“„ analyze_and_segment_document(paper_dir: str, force_refresh: bool = False)
   Purpose: Analyzes document structure and creates intelligent segments
   - Detects document type (research paper, technical doc, algorithm-focused, etc.)
   - Selects optimal segmentation strategy based on content analysis
   - Creates semantic segments preserving algorithm and concept integrity
   - Stores segmentation index for efficient retrieval
   - Returns: JSON with segmentation status, strategy used, and segment count
```

çœ‹çœ‹ä¸­æ–‡ï¼š

```
ğŸ“„ analyze_and_segment_document(paper_dir: str, force_refresh: bool = False)
ç”¨é€”ï¼šåˆ†ææ–‡æ¡£ç»“æ„å¹¶åˆ›å»ºæ™ºèƒ½åˆ†æ®µ
- æ£€æµ‹æ–‡æ¡£ç±»å‹ï¼ˆç ”ç©¶è®ºæ–‡ã€æŠ€æœ¯æ–‡æ¡£ã€ç®—æ³•ç±»æ–‡æ¡£ç­‰ï¼‰
- æ ¹æ®å†…å®¹åˆ†æé€‰æ‹©æœ€ä½³åˆ†æ®µç­–ç•¥
- åˆ›å»ºè¯­ä¹‰åˆ†æ®µï¼Œä¿ç•™ç®—æ³•å’Œæ¦‚å¿µçš„å®Œæ•´æ€§
- å­˜å‚¨åˆ†æ®µç´¢å¼•ä»¥ä¾¿é«˜æ•ˆæ£€ç´¢
- è¿”å›ï¼šåŒ…å«åˆ†æ®µçŠ¶æ€ã€æ‰€ç”¨ç­–ç•¥å’Œåˆ†æ®µè®¡æ•°çš„ JSON æ•°æ®
```

è€æ ·å­ï¼Œæ¥æœ`@mcp.tool()`ï¼Œæ‰¾åˆ°`analyze_and_segment_document`å®ç°çš„åœ°æ–¹ï¼Œæ¥ä»ä¸Šå¾€ä¸‹çœ‹ï¼Œé¦–å…ˆè¿˜æ˜¯ç»å…¸å–å‡ºæ‰€æœ‰æ–‡ä»¶ï¼Œå¾ˆå¥‡æ€ªè¿™è¾¹éƒ½åªä¼šå¤„ç†ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼Œé‚£å¤šæ–‡ä»¶çš„æ„ä¹‰åœ¨å“ªé‡Œå‘¢ï¼Œæä¸æ‡‚

é¦–å…ˆæ˜¯å¦‚æœè€çš„é…ç½®æ–‡ä»¶å­˜åœ¨å°±åŠ è½½ï¼Œä¸å­˜åœ¨å°±è¯»å…¥è¦å¤„ç†çš„æ–‡ä»¶

é¦–å…ˆæ˜¯åˆ†ææ–‡ä»¶éƒ¨åˆ†ï¼š

``` python
# Analyze document
analyzer = DocumentAnalyzer()
doc_type, confidence = analyzer.analyze_document_type(content)
strategy = analyzer.detect_segmentation_strategy(content, doc_type)
```

`DocumentAnalyzer`ç±»ä¸­æ²¡æœ‰åˆå§‹åŒ–å‡½æ•°ï¼Œé‚£ä¹ˆæˆ‘ä»¬ç›´æ¥å¾€ä¸‹çœ‹`analyze_document_type`ï¼Œä»–é¦–å…ˆæŠŠå†…å®¹å°å†™äº†ï¼Œç„¶åå¼€å§‹è®¡ç®—æƒé‡ï¼Ÿ

æˆ‘ä»¬æ¥çœ‹çœ‹`_calculate_weighted_score`ï¼Œå¤§å®¶çœ‹äº†ä»¥åè‡ªè¡Œå¾€ä¸Šç¿»çœ‹çœ‹å°±çŸ¥é“äº†ï¼Œå…¶å®å°±æ˜¯åˆ¤æ–­è¯æ±‡å‡ºç°çš„æ¬¡æ•°æ¥è®¡ç®—æƒé‡

ç„¶åä¸‹é¢æœ‰ä¸€ä¸ªDetect semantic patterns of document typesï¼Œä¹Ÿå°±æ˜¯è¯­ä¹‰åˆ†ç±»ï¼Œæ¥çœ‹ä¸‹`_detect_pattern_score`ï¼Œè¿™ä¸ªå’Œ`_calculate_weighted_score`ç±»ä¼¼ï¼Œæˆ‘ä»¬å°±ä¸çœ‹äº†ï¼Œç„¶åä¸‹é¢å°±æ˜¯ç®—æ€»å¾—åˆ†ï¼Œç»™å‡ºåˆ†ç±»å’Œç½®ä¿¡åº¦

ç„¶åæ¥çœ‹ä¸€ä¸‹`detect_segmentation_strategy`ï¼Œè¿™é‡Œè¿˜æ˜¯å„ç§å…³é”®è¯æ£€ç´¢ï¼Œç„¶åç¡®å®šæ–‡æ¡£ç±»å‹

åœ¨å·¥å…·é‡Œç»§ç»­å¾€ä¸‹çœ‹ï¼Œåˆ›å»ºåˆ†å—çš„å…·ä½“é€»è¾‘ï¼š

``` python
# Create segments
segments = segmenter.segment_document(content, strategy)
```

æ¥çœ‹`segment_document`

``` python
def segment_document(self, content: str, strategy: str) -> List[DocumentSegment]:
    """
    Perform intelligent segmentation using the specified strategy
    """
    if strategy == "semantic_research_focused":
        return self._segment_research_paper_semantically(content)
    elif strategy == "algorithm_preserve_integrity":
        return self._segment_preserve_algorithm_integrity(content)
    elif strategy == "concept_implementation_hybrid":
        return self._segment_concept_implementation_hybrid(content)
    elif strategy == "semantic_chunking_enhanced":
        return self._segment_by_enhanced_semantic_chunks(content)
    elif strategy == "content_aware_segmentation":
        return self._segment_content_aware(content)
    else:
        # Compatibility with legacy strategies
        return self._segment_by_enhanced_semantic_chunks(content)
```

è¿™ä¸ªå¾ˆæœ´ç´ äº†ï¼Œå¤§å®¶è‡ªå·±çœ‹å§ï¼Œæˆ‘ä»¬æƒå½“åˆ†å—è·‘å®Œäº†

è€æ ·å­ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªphaseç»“æŸæ‰“ä¸ªæ–­ç‚¹ï¼Œçœ‹çœ‹è¿è¡Œåˆ°è¿™é‡Œå‘ç”Ÿäº†ä»€ä¹ˆ

ç»“æœæ¯›éƒ½æ²¡ç»™æˆ‘åˆ†ç‰‡å‡ºæ¥ï¼Œçœ‹çœ‹`document_index.json`

``` json
{
  "document_path": "/Users/dinglizhi/Desktop/coderead/DeepCode/deepcode_lab/papers/1/1.md",
  "document_type": "research_paper",
  "segmentation_strategy": "concept_implementation_hybrid",
  "total_segments": 0,
  "total_chars": 59608,
  "segments": [],
  "created_at": "2025-10-11T15:04:27.466987"
}
```

æˆ‘ä»¬å°±å½“ä»–æ²¡åˆ†ç‰‡è¿™ä¸ªåŠŸèƒ½å¥½å§

### phase4

